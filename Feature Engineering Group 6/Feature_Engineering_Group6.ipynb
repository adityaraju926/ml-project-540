{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8112f1-9b8e-40e8-9896-566c72b704f4",
   "metadata": {},
   "source": [
    "**Feature Engineering**   \n",
    "Fintech 540 - Machine Learning  \n",
    "Project - Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1f3bb-7c06-450f-9699-8c95a627bdb7",
   "metadata": {},
   "source": [
    "**Initial Data:** RV_March2024.xlsx \n",
    "\n",
    "**Objective:**  \n",
    "Transform raw volatility measures into a feature-rich dataset for predictive modeling.\n",
    "\n",
    "**Data Source:**  \n",
    "- 30 Dow Jones stocks\n",
    "- 5,346 trading days (2003-2024)\n",
    "- 10 volatility measures per stock (RV, BPV, Good, Bad, RQ at 1-min and 5-min frequencies)\n",
    "\n",
    "**Feature Engineering Strategy:**\n",
    "1. **Temporal Features**: Lags, rolling statistics, momentum indicators\n",
    "2. **Cross-Sectional Features**: Relative volatility, market-wide measures\n",
    "3. **Variance Decomposition Features**: Jump indicators, continuous vs discontinuous ratios\n",
    "4. **Frequency Features**: 1-min vs 5-min relationships\n",
    "5. **Data Quality Features**: Missing data indicators, data availability flags\n",
    " \n",
    "**Output:**  \n",
    "Clean, feature-rich dataset ready for machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921df28-cb03-4d88-98cb-562b02fdacdd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "379c241c-af2c-4dad-a7e1-125d4149f81c",
   "metadata": {},
   "source": [
    "**1. Environment Setup and Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d23906-58b3-4779-b280-74d5f2fe316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "FEATURE ENGINEERING PIPELINE - RV MARCH 2024\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(\"FEATURE ENGINEERING PIPELINE - RV MARCH 2024\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bafaf-1abc-41f2-a634-d2a2d382820f",
   "metadata": {},
   "source": [
    "**2. Load Raw Data**  \n",
    "We load all sheets from the Excel file and structure them properly. Missing data (encoded as zeros) will be handled explicitly in the feature engineering process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b36ab62-1604-4687-8800-bab3b61ab4a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/RV_March2024.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m DATA_PATH = \u001b[33m\"\u001b[39m\u001b[33m../data/RV_March2024.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load reference sheets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dates_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m companies_df = pd.read_excel(DATA_PATH, sheet_name=\u001b[33m\"\u001b[39m\u001b[33mCompanies\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Convert dates + company list\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/RV_March2024.xlsx'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bd8c47-ff7a-4a2a-99b1-46368cae1c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RV_March2024.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load reference sheets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dates_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRV_March2024.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m companies_df = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33mRV_March2024.xlsx\u001b[39m\u001b[33m\"\u001b[39m, sheet_name=\u001b[33m\"\u001b[39m\u001b[33mCompanies\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Get lists for iteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ds_general/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'RV_March2024.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load reference sheets\n",
    "dates_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Dates\", header=None)\n",
    "companies_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Companies\", header=None)\n",
    "\n",
    "# Get lists for iteration\n",
    "dates_list = pd.to_datetime(dates_df[0], format='%d-%b-%Y')\n",
    "companies_list = companies_df[0].tolist()\n",
    "\n",
    "print(f\"Data Period: {dates_list.min().date()} to {dates_list.max().date()}\")\n",
    "print(f\"Number of trading days: {len(dates_list)}\")\n",
    "print(f\"Number of companies: {len(companies_list)}\")\n",
    "print(f\"\\nCompanies: {', '.join(companies_list)}\")\n",
    "\n",
    "# Load all volatility measure sheets\n",
    "sheet_names = ['RV', 'BPV', 'Good', 'Bad', 'RQ', 'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5']\n",
    "data_dict = {}\n",
    "\n",
    "print(f\"\\nLoading {len(sheet_names)} data sheets...\")\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=sheet, header=None)\n",
    "    df.columns = companies_list\n",
    "    df.index = dates_list\n",
    "    df.index.name = 'Date'\n",
    "    data_dict[sheet] = df\n",
    "    print(f\"  ✓ {sheet:8s}: {df.shape}\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f0073-0024-4199-a0ac-d5310fca2b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4b0ba-f595-4a85-bd53-d9a03e75c18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3beb9-a43c-4851-9a35-f052b44627f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946aa79-e270-4b57-89fd-60c7323266e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7736e-b79d-44db-ac79-d5551cba2797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8c897-14a2-46d7-b178-eb3e7e2b1c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3a9fe-460f-4e35-a06f-920cef53e7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0153fe-8881-41f8-9137-c30b6331d8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832df2e-3761-4614-8efc-ee9827e4641b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a921dc2d-4b3d-473f-8c1e-eaa084783b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# %% [markdown]\\n# # Feature Engineering: High-Frequency Volatility Data\\n# ## RV_March2024.xlsx - Building Predictive Features\\n# \\n# **Objective:**  \\n# Transform raw volatility measures into a feature-rich dataset for predictive modeling.\\n# \\n# **Data Source:**  \\n# - 30 Dow Jones stocks\\n# - 5,346 trading days (2003-2024)\\n# - 10 volatility measures per stock (RV, BPV, Good, Bad, RQ at 1-min and 5-min frequencies)\\n# \\n# **Feature Engineering Strategy:**\\n# 1. **Temporal Features**: Lags, rolling statistics, momentum indicators\\n# 2. **Cross-Sectional Features**: Relative volatility, market-wide measures\\n# 3. **Variance Decomposition Features**: Jump indicators, continuous vs discontinuous ratios\\n# 4. **Frequency Features**: 1-min vs 5-min relationships\\n# 5. **Data Quality Features**: Missing data indicators, data availability flags\\n# \\n# **Output:**  \\n# Clean, feature-rich dataset ready for machine learning models.\\n\\n# %% [markdown]\\n# ## 1. Environment Setup and Data Loading\\n\\n# %%\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Set display options\\npd.set_option(\\'display.max_columns\\', None)\\npd.set_option(\\'display.precision\\', 4)\\n\\nprint(\"=\"*70)\\nprint(\"FEATURE ENGINEERING PIPELINE - RV MARCH 2024\")\\nprint(\"=\"*70)\\n\\n# %% [markdown]\\n# ## 2. Load Raw Data\\n# \\n# We load all sheets from the Excel file and structure them properly. Missing data (encoded as zeros) will be handled explicitly in the feature engineering process.\\n\\n# %%\\n# Load reference sheets\\ndates_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Dates\", header=None)\\ncompanies_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Companies\", header=None)\\n\\n# Get lists for iteration\\ndates_list = pd.to_datetime(dates_df[0], format=\\'%d-%b-%Y\\')\\ncompanies_list = companies_df[0].tolist()\\n\\nprint(f\"Data Period: {dates_list.min().date()} to {dates_list.max().date()}\")\\nprint(f\"Number of trading days: {len(dates_list)}\")\\nprint(f\"Number of companies: {len(companies_list)}\")\\nprint(f\"\\nCompanies: {\\', \\'.join(companies_list)}\")\\n\\n# Load all volatility measure sheets\\nsheet_names = [\\'RV\\', \\'BPV\\', \\'Good\\', \\'Bad\\', \\'RQ\\', \\'RV_5\\', \\'BPV_5\\', \\'Good_5\\', \\'Bad_5\\', \\'RQ_5\\']\\ndata_dict = {}\\n\\nprint(f\"\\nLoading {len(sheet_names)} data sheets...\")\\nfor sheet in sheet_names:\\n    df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=sheet, header=None)\\n    df.columns = companies_list\\n    df.index = dates_list\\n    df.index.name = \\'Date\\'\\n    data_dict[sheet] = df\\n    print(f\"  ✓ {sheet:8s}: {df.shape}\")\\n\\nprint(\"\\nData loaded successfully!\")\\n\\n# %% [markdown]\\n# ## 3. Data Preprocessing and Cleaning\\n# \\n# **Key Steps:**\\n# - Replace zeros with NaN (missing data encoding)\\n# - Create a master dataset combining all measures\\n# - Handle missing data systematically\\n# - Validate data integrity\\n\\n# %%\\n# Replace zeros with NaN across all sheets\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"DATA PREPROCESSING\")\\nprint(\"=\"*70)\\n\\nfor sheet_name, df in data_dict.items():\\n    data_dict[sheet_name] = df.replace(0, np.nan)\\n    missing_pct = (df.isna().sum().sum() / (df.shape[0] * df.shape[1])) * 100\\n    print(f\"{sheet_name:8s}: {missing_pct:.2f}% missing data\")\\n\\n# Create long-format master dataset\\nprint(\"\\nCreating master dataset in long format...\")\\n\\nmaster_data = []\\nfor ticker in companies_list:\\n    ticker_data = pd.DataFrame({\\n        \\'Date\\': dates_list,\\n        \\'Ticker\\': ticker\\n    })\\n    \\n    # Add all measures for this ticker\\n    for sheet_name in sheet_names:\\n        ticker_data[sheet_name] = data_dict[sheet_name][ticker].values\\n    \\n    master_data.append(ticker_data)\\n\\ndf_master = pd.concat(master_data, ignore_index=True)\\n\\nprint(f\"\\nMaster dataset created: {df_master.shape}\")\\nprint(f\"Total observations: {len(df_master):,}\")\\nprint(f\"Features: Date, Ticker + {len(sheet_names)} volatility measures\")\\n\\n# Display sample\\nprint(\"\\nSample of master dataset:\")\\nprint(df_master.head(10))\\n\\n# %% [markdown]\\n# ## 4. Feature Creation - Temporal Features\\n# \\n# **Temporal features capture time-series dynamics:**\\n# - **Lags**: Past values (t-1, t-5, t-20)\\n# - **Rolling statistics**: Moving averages, standard deviations\\n# - **Momentum**: Rate of change indicators\\n# - **Trend**: Direction and strength of volatility trends\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING TEMPORAL FEATURES\")\\nprint(\"=\"*70)\\n\\n# Sort by ticker and date for proper time series operations\\ndf_master = df_master.sort_values([\\'Ticker\\', \\'Date\\']).reset_index(drop=True)\\n\\n# Define lag periods\\nlag_periods = [1, 5, 10, 20]  # 1 day, 1 week, 2 weeks, 1 month\\n\\n# Define rolling windows\\nrolling_windows = [5, 20, 60]  # 1 week, 1 month, 3 months\\n\\n# Create lag features for RV (primary target)\\nprint(\"\\nCreating lag features for RV...\")\\nfor lag in lag_periods:\\n    df_master[f\\'RV_lag{lag}\\'] = df_master.groupby(\\'Ticker\\')[\\'RV\\'].shift(lag)\\n    print(f\"  ✓ RV_lag{lag}\")\\n\\n# Create rolling statistics\\nprint(\"\\nCreating rolling statistics...\")\\nfor window in rolling_windows:\\n    # Rolling mean\\n    df_master[f\\'RV_roll_mean_{window}\\'] = (\\n        df_master.groupby(\\'Ticker\\')[\\'RV\\']\\n        .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\\n    )\\n    \\n    # Rolling std\\n    df_master[f\\'RV_roll_std_{window}\\'] = (\\n        df_master.groupby(\\'Ticker\\')[\\'RV\\']\\n        .transform(lambda x: x.rolling(window=window, min_periods=1).std())\\n    )\\n    \\n    # Rolling min/max\\n    df_master[f\\'RV_roll_min_{window}\\'] = (\\n        df_master.groupby(\\'Ticker\\')[\\'RV\\']\\n        .transform(lambda x: x.rolling(window=window, min_periods=1).min())\\n    )\\n    \\n    df_master[f\\'RV_roll_max_{window}\\'] = (\\n        df_master.groupby(\\'Ticker\\')[\\'RV\\']\\n        .transform(lambda x: x.rolling(window=window, min_periods=1).max())\\n    )\\n    \\n    print(f\"  ✓ Rolling features (window={window})\")\\n\\n# Momentum features\\nprint(\"\\nCreating momentum features...\")\\ndf_master[\\'RV_momentum_5\\'] = df_master[\\'RV\\'] / df_master[\\'RV_lag5\\'] - 1\\ndf_master[\\'RV_momentum_20\\'] = df_master[\\'RV\\'] / df_master[\\'RV_lag20\\'] - 1\\n\\n# Volatility of volatility\\ndf_master[\\'RV_volatility_20\\'] = (\\n    df_master.groupby(\\'Ticker\\')[\\'RV\\']\\n    .transform(lambda x: x.rolling(window=20, min_periods=1).std())\\n)\\n\\nprint(f\"\\nTotal temporal features created: {len([c for c in df_master.columns if \\'lag\\' in c or \\'roll\\' in c or \\'momentum\\' in c])}\")\\n\\n# %% [markdown]\\n# ## 5. Feature Creation - Variance Decomposition Features\\n# \\n# **Decomposition features leverage the Good/Bad variance split:**\\n# - **Jump indicators**: Bad variance relative to total\\n# - **Continuous component strength**: Good variance ratios\\n# - **Jump detection signals**: Threshold-based indicators\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING VARIANCE DECOMPOSITION FEATURES\")\\nprint(\"=\"*70)\\n\\n# Good/Bad variance ratios\\nprint(\"\\nCreating variance decomposition ratios...\")\\n\\ndf_master[\\'Good_Bad_ratio\\'] = df_master[\\'Good\\'] / df_master[\\'Bad\\']\\ndf_master[\\'Bad_pct\\'] = df_master[\\'Bad\\'] / df_master[\\'RV\\']\\ndf_master[\\'Good_pct\\'] = df_master[\\'Good\\'] / df_master[\\'RV\\']\\n\\n# Verify decomposition (should be close to 1.0)\\ndf_master[\\'decomp_check\\'] = (df_master[\\'Good\\'] + df_master[\\'Bad\\']) / df_master[\\'RV\\']\\n\\nprint(\"  ✓ Good/Bad ratios\")\\nprint(\"  ✓ Bad percentage (jump indicator)\")\\nprint(\"  ✓ Good percentage (continuous component)\")\\n\\n# Jump detection features\\nprint(\"\\nCreating jump detection features...\")\\n\\n# Define jump threshold (Bad variance > 20% of total)\\nJUMP_THRESHOLD = 0.20\\n\\ndf_master[\\'jump_indicator\\'] = (df_master[\\'Bad_pct\\'] > JUMP_THRESHOLD).astype(int)\\n\\n# Rolling jump frequency\\ndf_master[\\'jump_freq_20\\'] = (\\n    df_master.groupby(\\'Ticker\\')[\\'jump_indicator\\']\\n    .transform(lambda x: x.rolling(window=20, min_periods=1).mean())\\n)\\n\\n# Jump intensity\\ndf_master[\\'jump_intensity\\'] = df_master[\\'Bad\\'] * df_master[\\'jump_indicator\\']\\n\\nprint(f\"  ✓ Jump indicator (threshold={JUMP_THRESHOLD})\")\\nprint(f\"  ✓ Rolling jump frequency\")\\nprint(f\"  ✓ Jump intensity\")\\n\\n# Compare 1-min vs 5-min decomposition\\nprint(\"\\nCreating cross-frequency decomposition features...\")\\n\\ndf_master[\\'Bad_pct_5\\'] = df_master[\\'Bad_5\\'] / df_master[\\'RV_5\\']\\ndf_master[\\'jump_diff_freq\\'] = df_master[\\'Bad_pct\\'] - df_master[\\'Bad_pct_5\\']\\n\\nprint(\"  ✓ Cross-frequency jump comparison\")\\n\\nprint(f\"\\nTotal decomposition features: {len([c for c in df_master.columns if \\'Good\\' in c or \\'Bad\\' in c or \\'jump\\' in c])}\")\\n\\n# %% [markdown]\\n# ## 6. Feature Creation - Cross-Sectional Features\\n# \\n# **Cross-sectional features compare a stock\\'s volatility to the market:**\\n# - **Relative volatility**: Stock RV vs market average\\n# - **Volatility rank**: Percentile ranking among stocks\\n# - **Market-wide measures**: Average, median, spread across all stocks\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING CROSS-SECTIONAL FEATURES\")\\nprint(\"=\"*70)\\n\\n# Calculate market-wide statistics for each date\\nprint(\"\\nCalculating market-wide statistics...\")\\n\\nmarket_stats = df_master.groupby(\\'Date\\').agg({\\n    \\'RV\\': [\\'mean\\', \\'median\\', \\'std\\', \\'min\\', \\'max\\'],\\n    \\'Bad_pct\\': [\\'mean\\', \\'median\\'],\\n    \\'jump_indicator\\': \\'sum\\'\\n}).reset_index()\\n\\n# Flatten column names\\nmarket_stats.columns = [\\'Date\\', \\'market_RV_mean\\', \\'market_RV_median\\', \\n                        \\'market_RV_std\\', \\'market_RV_min\\', \\'market_RV_max\\',\\n                        \\'market_Bad_pct_mean\\', \\'market_Bad_pct_median\\',\\n                        \\'market_jump_count\\']\\n\\n# Merge back to main dataset\\ndf_master = df_master.merge(market_stats, on=\\'Date\\', how=\\'left\\')\\n\\nprint(\"  ✓ Market-wide RV statistics\")\\nprint(\"  ✓ Market-wide jump statistics\")\\n\\n# Relative volatility features\\nprint(\"\\nCreating relative volatility features...\")\\n\\ndf_master[\\'RV_vs_market\\'] = df_master[\\'RV\\'] / df_master[\\'market_RV_mean\\']\\ndf_master[\\'RV_zscore\\'] = ((df_master[\\'RV\\'] - df_master[\\'market_RV_mean\\']) / \\n                          df_master[\\'market_RV_std\\'])\\n\\n# Volatility rank (percentile)\\ndf_master[\\'RV_rank\\'] = (\\n    df_master.groupby(\\'Date\\')[\\'RV\\']\\n    .rank(pct=True)\\n)\\n\\nprint(\"  ✓ Relative volatility vs market\")\\nprint(\"  ✓ Volatility z-score\")\\nprint(\"  ✓ Volatility rank (percentile)\")\\n\\n# Market dispersion\\ndf_master[\\'market_dispersion\\'] = df_master[\\'market_RV_max\\'] - df_master[\\'market_RV_min\\']\\ndf_master[\\'market_CV\\'] = df_master[\\'market_RV_std\\'] / df_master[\\'market_RV_mean\\']\\n\\nprint(\"  ✓ Market dispersion measures\")\\n\\nprint(f\"\\nTotal cross-sectional features: {len([c for c in df_master.columns if \\'market\\' in c or \\'rank\\' in c or \\'vs_market\\' in c])}\")\\n\\n# %% [markdown]\\n# ## 7. Feature Creation - Frequency Relationship Features\\n# \\n# **Leverage the relationship between 1-min and 5-min measures:**\\n# - **Microstructure noise**: 1-min vs 5-min differences\\n# - **Frequency ratios**: How measures scale across frequencies\\n# - **Consistency indicators**: Agreement between frequencies\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING FREQUENCY RELATIONSHIP FEATURES\")\\nprint(\"=\"*70)\\n\\n# 1-min vs 5-min ratios\\nprint(\"\\nCreating frequency ratio features...\")\\n\\ndf_master[\\'RV_freq_ratio\\'] = df_master[\\'RV\\'] / df_master[\\'RV_5\\']\\ndf_master[\\'BPV_freq_ratio\\'] = df_master[\\'BPV\\'] / df_master[\\'BPV_5\\']\\ndf_master[\\'Good_freq_ratio\\'] = df_master[\\'Good\\'] / df_master[\\'Good_5\\']\\ndf_master[\\'Bad_freq_ratio\\'] = df_master[\\'Bad\\'] / df_master[\\'Bad_5\\']\\n\\nprint(\"  ✓ RV frequency ratio (1-min/5-min)\")\\nprint(\"  ✓ BPV frequency ratio\")\\nprint(\"  ✓ Good/Bad frequency ratios\")\\n\\n# Microstructure noise indicator\\n# Theory: excess RV at 1-min suggests noise\\ndf_master[\\'microstructure_noise\\'] = df_master[\\'RV\\'] - df_master[\\'RV_5\\']\\n\\n# Frequency consistency\\ndf_master[\\'freq_consistency\\'] = np.abs(1 - df_master[\\'RV_freq_ratio\\'])\\n\\nprint(\"  ✓ Microstructure noise estimate\")\\nprint(\"  ✓ Frequency consistency measure\")\\n\\nprint(f\"\\nTotal frequency features: {len([c for c in df_master.columns if \\'freq\\' in c or \\'microstructure\\' in c])}\")\\n\\n# %% [markdown]\\n# ## 8. Feature Creation - Calendar and Time Features\\n# \\n# **Time-based features can capture seasonality and patterns:**\\n# - **Day of week**: Monday effect, Friday effect\\n# - **Month**: Seasonal patterns\\n# - **Quarter**: Quarterly earnings effects\\n# - **Year**: Long-term trends\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING CALENDAR FEATURES\")\\nprint(\"=\"*70)\\n\\n# Extract time components\\ndf_master[\\'year\\'] = df_master[\\'Date\\'].dt.year\\ndf_master[\\'month\\'] = df_master[\\'Date\\'].dt.month\\ndf_master[\\'quarter\\'] = df_master[\\'Date\\'].dt.quarter\\ndf_master[\\'day_of_week\\'] = df_master[\\'Date\\'].dt.dayofweek  # Monday=0, Sunday=6\\ndf_master[\\'day_of_month\\'] = df_master[\\'Date\\'].dt.day\\ndf_master[\\'week_of_year\\'] = df_master[\\'Date\\'].dt.isocalendar().week\\n\\nprint(\"  ✓ Year, Month, Quarter\")\\nprint(\"  ✓ Day of week, Day of month\")\\nprint(\"  ✓ Week of year\")\\n\\n# Is Monday/Friday (known volatility patterns)\\ndf_master[\\'is_monday\\'] = (df_master[\\'day_of_week\\'] == 0).astype(int)\\ndf_master[\\'is_friday\\'] = (df_master[\\'day_of_week\\'] == 4).astype(int)\\n\\n# Month-end effect (last 3 days of month)\\ndf_master[\\'is_month_end\\'] = (df_master[\\'day_of_month\\'] >= 28).astype(int)\\n\\nprint(\"  ✓ Monday/Friday indicators\")\\nprint(\"  ✓ Month-end indicator\")\\n\\nprint(f\"\\nTotal calendar features: {len([c for c in df_master.columns if c in [\\'year\\', \\'month\\', \\'quarter\\', \\'day_of_week\\', \\'day_of_month\\', \\'week_of_year\\', \\'is_monday\\', \\'is_friday\\', \\'is_month_end\\']])}\")\\n\\n# %% [markdown]\\n# ## 9. Feature Creation - Data Quality Features\\n# \\n# **Track data availability and quality:**\\n# - **Missing data indicators**: Whether original data was missing\\n# - **Data completeness scores**: How much recent data is available\\n# - **Imputation flags**: Mark imputed values\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING DATA QUALITY FEATURES\")\\nprint(\"=\"*70)\\n\\n# Create missingness indicators (before any imputation)\\nprint(\"\\nCreating missingness indicators...\")\\n\\ndf_master[\\'RV_is_missing\\'] = df_master[\\'RV\\'].isna().astype(int)\\ndf_master[\\'BPV_is_missing\\'] = df_master[\\'BPV\\'].isna().astype(int)\\ndf_master[\\'Good_is_missing\\'] = df_master[\\'Good\\'].isna().astype(int)\\ndf_master[\\'Bad_is_missing\\'] = df_master[\\'Bad\\'].isna().astype(int)\\n\\n# Data completeness score (% of non-missing in last 20 days)\\ndf_master[\\'data_completeness_20\\'] = (\\n    df_master.groupby(\\'Ticker\\')[\\'RV_is_missing\\']\\n    .transform(lambda x: 1 - x.rolling(window=20, min_periods=1).mean())\\n)\\n\\nprint(\"  ✓ Missingness indicators for key measures\")\\nprint(\"  ✓ Data completeness score (20-day)\")\\n\\n# Consecutive missing days\\ndf_master[\\'consec_missing\\'] = (\\n    df_master.groupby(\\'Ticker\\')[\\'RV_is_missing\\']\\n    .transform(lambda x: x.groupby((x != x.shift()).cumsum()).cumsum())\\n)\\n\\nprint(\"  ✓ Consecutive missing days counter\")\\n\\nprint(f\"\\nTotal data quality features: {len([c for c in df_master.columns if \\'missing\\' in c or \\'completeness\\' in c])}\")\\n\\n# %% [markdown]\\n# ## 10. Handle Missing Data\\n# \\n# **Strategy:**\\n# 1. **Forward fill**: For short gaps (≤3 days)\\n# 2. **Linear interpolation**: For medium gaps (4-10 days)\\n# 3. **Leave as NaN**: For long gaps (>10 days) - will drop these rows for modeling\\n# \\n# Note: We\\'ve already created missingness indicators, so we preserve information about what was imputed.\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"HANDLING MISSING DATA\")\\nprint(\"=\"*70)\\n\\n# Count missing before imputation\\nmissing_before = df_master[\\'RV\\'].isna().sum()\\nprint(f\"\\nMissing RV values before imputation: {missing_before:,} ({missing_before/len(df_master)*100:.2f}%)\")\\n\\n# Apply imputation strategy by ticker\\nprint(\"\\nApplying imputation strategy...\")\\n\\nimputation_cols = [\\'RV\\', \\'BPV\\', \\'Good\\', \\'Bad\\', \\'RQ\\', \\'RV_5\\', \\'BPV_5\\', \\'Good_5\\', \\'Bad_5\\', \\'RQ_5\\']\\n\\nfor ticker in companies_list:\\n    ticker_mask = df_master[\\'Ticker\\'] == ticker\\n    \\n    for col in imputation_cols:\\n        # Forward fill (limit to 3 periods)\\n        df_master.loc[ticker_mask, col] = (\\n            df_master.loc[ticker_mask, col].fillna(method=\\'ffill\\', limit=3)\\n        )\\n        \\n        # Interpolate remaining gaps (limit to 10 periods)\\n        df_master.loc[ticker_mask, col] = (\\n            df_master.loc[ticker_mask, col].interpolate(method=\\'linear\\', limit=10)\\n        )\\n\\nmissing_after = df_master[\\'RV\\'].isna().sum()\\nprint(f\"Missing RV values after imputation: {missing_after:,} ({missing_after/len(df_master)*100:.2f}%)\")\\nprint(f\"Imputation recovered: {missing_before - missing_after:,} values ({(missing_before-missing_after)/missing_before*100:.2f}%)\")\\n\\n# Drop rows with remaining missing values in key columns\\nprint(\"\\nDropping rows with remaining missing values in key measures...\")\\nrows_before = len(df_master)\\ndf_master = df_master.dropna(subset=[\\'RV\\', \\'Good\\', \\'Bad\\'])\\nrows_after = len(df_master)\\nprint(f\"Rows dropped: {rows_before - rows_after:,} ({(rows_before-rows_after)/rows_before*100:.2f}%)\")\\nprint(f\"Final dataset size: {rows_after:,} observations\")\\n\\n# %% [markdown]\\n# ## 11. Feature Summary and Validation\\n# \\n# **Review all created features and validate data integrity.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FEATURE SUMMARY\")\\nprint(\"=\"*70)\\n\\n# Categorize features\\nfeature_categories = {\\n    \\'Identifier\\': [\\'Date\\', \\'Ticker\\'],\\n    \\'Original Measures\\': sheet_names,\\n    \\'Temporal\\': [c for c in df_master.columns if \\'lag\\' in c or \\'roll\\' in c or \\'momentum\\' in c or \\'volatility\\' in c],\\n    \\'Decomposition\\': [c for c in df_master.columns if (\\'Good\\' in c or \\'Bad\\' in c or \\'jump\\' in c or \\'decomp\\' in c) and c not in sheet_names],\\n    \\'Cross-Sectional\\': [c for c in df_master.columns if \\'market\\' in c or \\'rank\\' in c or \\'vs_market\\' in c or \\'dispersion\\' in c or \\'CV\\' in c],\\n    \\'Frequency\\': [c for c in df_master.columns if \\'freq\\' in c or \\'microstructure\\' in c],\\n    \\'Calendar\\': [\\'year\\', \\'month\\', \\'quarter\\', \\'day_of_week\\', \\'day_of_month\\', \\'week_of_year\\', \\'is_monday\\', \\'is_friday\\', \\'is_month_end\\'],\\n    \\'Data Quality\\': [c for c in df_master.columns if \\'missing\\' in c or \\'completeness\\' in c or \\'consec\\' in c]\\n}\\n\\ntotal_features = 0\\nfor category, features in feature_categories.items():\\n    print(f\"\\n{category}:\")\\n    print(f\"  Count: {len(features)}\")\\n    if len(features) <= 10:\\n        print(f\"  Features: {\\', \\'.join(features)}\")\\n    else:\\n        print(f\"  Features: {\\', \\'.join(features[:5])} ... (showing 5/{len(features)})\")\\n    total_features += len(features)\\n\\nprint(f\"\\n{\\'=\\'*70}\")\\nprint(f\"TOTAL FEATURES: {total_features}\")\\nprint(f\"{\\'=\\'*70}\")\\n\\n# Display final dataset info\\nprint(\"\\nFinal Dataset Structure:\")\\nprint(df_master.info(verbose=False))\\n\\nprint(\"\\nSample of engineered features:\")\\nsample_cols = [\\'Date\\', \\'Ticker\\', \\'RV\\', \\'RV_lag1\\', \\'RV_roll_mean_20\\', \\n               \\'Bad_pct\\', \\'jump_indicator\\', \\'RV_vs_market\\', \\'RV_rank\\']\\nprint(df_master[sample_cols].head(10))\\n\\n# %% [markdown]\\n# ## 12. Data Validation and Quality Checks\\n# \\n# **Verify data integrity before saving for modeling.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"DATA VALIDATION\")\\nprint(\"=\"*70)\\n\\n# Check for infinite values\\nprint(\"\\nChecking for infinite values...\")\\ninf_cols = df_master.columns[df_master.isin([np.inf, -np.inf]).any()].tolist()\\nif inf_cols:\\n    print(f\"  ⚠️  Found infinite values in: {\\', \\'.join(inf_cols)}\")\\n    # Replace infinite values with NaN\\n    df_master.replace([np.inf, -np.inf], np.nan, inplace=True)\\n    print(\"  ✓ Infinite values replaced with NaN\")\\nelse:\\n    print(\"  ✓ No infinite values found\")\\n\\n# Check for remaining missing values\\nprint(\"\\nChecking for missing values...\")\\nmissing_summary = df_master.isna().sum()\\nmissing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\\nif len(missing_summary) > 0:\\n    print(f\"  Columns with missing values: {len(missing_summary)}\")\\n    print(missing_summary.head(10))\\nelse:\\n    print(\"  ✓ No missing values in core features\")\\n\\n# Validate variance decomposition\\nprint(\"\\nValidating variance decomposition...\")\\ndecomp_error = (df_master[\\'Good\\'] + df_master[\\'Bad\\'] - df_master[\\'RV\\']).abs()\\nprint(f\"  Mean absolute error: {decomp_error.mean():.6f}\")\\nprint(f\"  Max absolute error: {decomp_error.max():.6f}\")\\nif decomp_error.mean() < 0.01:\\n    print(\"  ✓ Decomposition validation passed\")\\nelse:\\n    print(\"  ⚠️  Decomposition has larger than expected errors\")\\n\\n# Validate date continuity\\nprint(\"\\nValidating temporal continuity...\")\\nfor ticker in companies_list[:3]:  # Check first 3 tickers\\n    ticker_dates = df_master[df_master[\\'Ticker\\'] == ticker][\\'Date\\'].sort_values()\\n    date_gaps = ticker_dates.diff().dt.days.dropna()\\n    max_gap = date_gaps.max()\\n    if max_gap > 5:  # More than 5 days suggests missing dates\\n        print(f\"  ⚠️  {ticker}: Max gap = {max_gap} days\")\\nprint(\"  ✓ Temporal continuity check complete\")\\n\\n# Check feature distributions\\nprint(\"\\nFeature distribution summary (RV):\")\\nprint(df_master[\\'RV\\'].describe())\\n\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"VALIDATION COMPLETE\")\\nprint(\"=\"*70)\\n\\n# %% [markdown]\\n# ## 13. Train/Validation/Test Split\\n# \\n# **Create time-based splits for modeling:**\\n# - **Training**: 2003-2018 (60%)\\n# - **Validation**: 2019-2021 (20%)\\n# - **Test**: 2022-2024 (20%)\\n# \\n# Time-based split prevents data leakage and simulates real-world forecasting.\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"CREATING TRAIN/VALIDATION/TEST SPLITS\")\\nprint(\"=\"*70)\\n\\n# Define split dates\\ntrain_end = \\'2018-12-31\\'\\nval_end = \\'2021-12-31\\'\\n\\n# Create splits\\ntrain_data = df_master[df_master[\\'Date\\'] <= train_end].copy()\\nval_data = df_master[(df_master[\\'Date\\'] > train_end) & (df_master[\\'Date\\'] <= val_end)].copy()\\ntest_data = df_master[df_master[\\'Date\\'] > val_end].copy()\\n\\nprint(f\"\\nTraining Set:\")\\nprint(f\"  Date range: {train_data[\\'Date\\'].min().date()} to {train_data[\\'Date\\'].max().date()}\")\\nprint(f\"  Observations: {len(train_data):,}\")\\nprint(f\"  Companies: {train_data[\\'Ticker\\'].nunique()}\")\\n\\nprint(f\"\\nValidation Set:\")\\nprint(f\"  Date range: {val_data[\\'Date\\'].min().date()} to {val_data[\\'Date\\'].max().date()}\")\\nprint(f\"  Observations: {len(val_data):,}\")\\nprint(f\"  Companies: {val_data[\\'Ticker\\'].nunique()}\")\\n\\nprint(f\"\\nTest Set:\")\\nprint(f\"  Date range: {test_data[\\'Date\\'].min().date()} to {test_data[\\'Date\\'].max().date()}\")\\nprint(f\"  Observations: {len(test_data):,}\")\\nprint(f\"  Companies: {test_data[\\'Ticker\\'].nunique()}\")\\n\\n# Visualize split\\nprint(\"\\nDistribution across splits:\")\\nsplit_dist = pd.DataFrame({\\n    \\'Split\\': [\\'Train\\', \\'Validation\\', \\'Test\\'],\\n    \\'Count\\': [len(train_data), len(val_data), len(test_data)],\\n    \\'Percentage\\': [len(train_data)/len(df_master)*100, \\n                   len(val_data)/len(df_master)*100,\\n                   len(test_data)/len(df_master)*100]\\n})\\nprint(split_dist)\\n\\n# %% [markdown]\\n# ## 14. Save Engineered Features\\n# \\n# **Save the feature-engineered dataset for modeling:**\\n# - Full dataset (all features)\\n# - Train/validation/test splits\\n# - Feature metadata\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"SAVING ENGINEERED DATASETS\")\\nprint(\"=\"*70)\\n\\n# Create output directory if it doesn\\'t exist\\nimport os\\nos.makedirs(\\'data/engineered\\', exist_ok=True)\\n\\n# Save full dataset\\nprint(\"\\nSaving full engineered dataset...\")\\ndf_master.to_parquet(\\'data/engineered/rv_features_full.parquet\\', index=False)\\ndf_master.to_csv(\\'data/engineered/rv_features_full.csv\\', index=False)\\nprint(\"  ✓ rv_features_full.parquet\")\\nprint(\"  ✓ rv_features_full.csv\")\\n\\n# Save splits\\nprint(\"\\nSaving train/val/test splits...\")\\ntrain_data.to_parquet(\\'data/engineered/rv_features_train.parquet\\', index=False)\\nval_data.to_parquet(\\'data/engineered/rv_features_val.parquet\\', index=False)\\ntest_data.to_parquet(\\'data/engineered/rv_features_test.parquet\\', index=False)\\nprint(\"  ✓ rv_features_train.parquet\")\\nprint(\"  ✓ rv_features_val.parquet\")\\nprint(\"  ✓ rv_features_test.parquet\")\\n\\n# Save feature metadata\\nprint(\"\\nSaving feature metadata...\")\\nfeature_metadata = pd.DataFrame({\\n    \\'Feature\\': df_master.columns,\\n    \\'Dtype\\': df_master.dtypes.values,\\n    \\'Missing_Count\\': df_master.isna().sum().values,\\n    \\'Missing_Pct\\': (df_master.isna().sum().values / len(df_master) * 100).round(2)\\n})\\n\\n# Add category information\\nfeature_metadata[\\'Category\\'] = \\'Other\\'\\nfor category, features in feature_categories.items():\\n    feature_metadata.loc[feature_metadata[\\'Feature\\'].isin(features), \\'Category\\'] = category\\n\\nfeature_metadata.to_csv(\\'data/engineered/feature_metadata.csv\\', index=False)\\nprint(\"  ✓ feature_metadata.csv\")\\n\\n# Save feature category mapping\\ncategory_summary = pd.DataFrame([\\n    {\\'Category\\': cat, \\'Count\\': len(feats), \\'Features\\': \\', \\'.join(feats[:3]) + \\'...\\' if len(feats) > 3 else \\', \\'.join(feats)}\\n    for cat, feats in feature_categories.items()\\n])\\ncategory_summary.to_csv(\\'data/engineered/feature_categories.csv\\', index=False)\\nprint(\"  ✓ feature_categories.csv\")\\n\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"ALL FILES SAVED SUCCESSFULLY\")\\nprint(\"=\"*70)\\nprint(f\"\\nOutput location: data/engineered/\")\\nprint(f\"Files created: 8\")\\nprint(f\"  - Full dataset: parquet + csv\")\\nprint(f\"  - Train/val/test splits: 3 parquet files\")\\nprint(f\"  - Metadata: 2 csv files\")\\n\\n# %% [markdown]\\n# ## 15. Feature Importance Analysis (Preview)\\n# \\n# **Quick analysis of feature correlations with target (RV):**\\n# This helps us understand which features might be most predictive.\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FEATURE CORRELATION ANALYSIS\")\\nprint(\"=\"*70)\\n\\n# Select numerical features (exclude identifiers and categorical)\\nnumerical_features = df_master.select_dtypes(include=[np.number]).columns.tolist()\\nexclude_cols = [\\'year\\', \\'month\\', \\'quarter\\', \\'day_of_week\\', \\'day_of_month\\', \\'week_of_year\\']\\nnumerical_features = [f for f in numerical_features if f not in exclude_cols]\\n\\n# Calculate correlation with RV (target)\\ncorrelations = df_master[numerical_features].corr()[\\'RV\\'].sort_values(ascending=False)\\n\\nprint(\"\\nTop 20 Features Most Correlated with RV:\")\\nprint(correlations.head(20))\\n\\nprint(\"\\nBottom 10 Features (Least Correlated with RV):\")\\nprint(correlations.tail(10))\\n\\n# Visualize top correlations\\nfig, ax = plt.subplots(figsize=(10, 8))\\ntop_features = correlations.head(21)[1:]  # Exclude RV itself\\ntop_features.plot(kind=\\'barh\\', ax=ax, color=\\'steelblue\\')\\nax.set_xlabel(\\'Correlation with RV\\')\\nax.set_title(\\'Top 20 Features Correlated with Realized Variance (RV)\\', fontsize=14, fontweight=\\'bold\\')\\nax.grid(axis=\\'x\\', alpha=0.3)\\nplt.tight_layout()\\nplt.savefig(\\'data/engineered/feature_correlations.png\\', dpi=150, bbox_inches=\\'tight\\')\\nplt.show()\\nprint(\"\\n  ✓ Correlation plot saved: feature_correlations.png\")\\n\\n# %% [markdown]\\n# ## 16. Feature Distribution Visualization\\n# \\n# **Visualize distributions of key engineered features.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FEATURE DISTRIBUTION VISUALIZATION\")\\nprint(\"=\"*70)\\n\\n# Select key features to visualize\\nkey_features = [\\n    \\'RV\\', \\'RV_lag1\\', \\'RV_roll_mean_20\\', \\'Bad_pct\\', \\n    \\'jump_indicator\\', \\'RV_vs_market\\', \\'RV_rank\\', \\'microstructure_noise\\'\\n]\\n\\nfig, axes = plt.subplots(4, 2, figsize=(14, 16))\\naxes = axes.flatten()\\n\\nfor idx, feature in enumerate(key_features):\\n    if feature in df_master.columns:\\n        data = df_master[feature].dropna()\\n        \\n        # Plot histogram\\n        axes[idx].hist(data, bins=50, alpha=0.7, color=\\'steelblue\\', edgecolor=\\'black\\')\\n        axes[idx].set_title(f\\'Distribution of {feature}\\', fontsize=11, fontweight=\\'bold\\')\\n        axes[idx].set_xlabel(feature)\\n        axes[idx].set_ylabel(\\'Frequency\\')\\n        axes[idx].grid(axis=\\'y\\', alpha=0.3)\\n        \\n        # Add statistics\\n        mean_val = data.mean()\\n        median_val = data.median()\\n        axes[idx].axvline(mean_val, color=\\'red\\', linestyle=\\'--\\', linewidth=1.5, label=f\\'Mean: {mean_val:.2f}\\')\\n        axes[idx].axvline(median_val, color=\\'green\\', linestyle=\\'--\\', linewidth=1.5, label=f\\'Median: {median_val:.2f}\\')\\n        axes[idx].legend(fontsize=8)\\n\\nplt.tight_layout()\\nplt.savefig(\\'data/engineered/feature_distributions.png\\', dpi=150, bbox_inches=\\'tight\\')\\nplt.show()\\nprint(\"  ✓ Distribution plot saved: feature_distributions.png\")\\n\\n# %% [markdown]\\n# ## 17. Time Series Feature Visualization\\n# \\n# **Visualize temporal features for a sample ticker.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"TEMPORAL FEATURE VISUALIZATION\")\\nprint(\"=\"*70)\\n\\n# Select a sample ticker\\nsample_ticker = \\'AAPL\\'\\nsample_data = df_master[df_master[\\'Ticker\\'] == sample_ticker].sort_values(\\'Date\\')\\n\\n# Create visualization\\nfig, axes = plt.subplots(4, 1, figsize=(16, 12))\\n\\n# Plot 1: RV with moving averages\\naxes[0].plot(sample_data[\\'Date\\'], sample_data[\\'RV\\'], linewidth=0.8, alpha=0.7, label=\\'RV\\', color=\\'navy\\')\\naxes[0].plot(sample_data[\\'Date\\'], sample_data[\\'RV_roll_mean_20\\'], linewidth=1.5, label=\\'20-day MA\\', color=\\'red\\')\\naxes[0].plot(sample_data[\\'Date\\'], sample_data[\\'RV_roll_mean_60\\'], linewidth=1.5, label=\\'60-day MA\\', color=\\'green\\')\\naxes[0].set_ylabel(\\'Realized Variance\\')\\naxes[0].set_title(f\\'{sample_ticker}: Realized Variance with Moving Averages\\', fontsize=12, fontweight=\\'bold\\')\\naxes[0].legend(loc=\\'upper left\\')\\naxes[0].grid(True, alpha=0.3)\\n\\n# Plot 2: Jump indicators\\naxes[1].fill_between(sample_data[\\'Date\\'], 0, sample_data[\\'Bad_pct\\'], alpha=0.6, color=\\'red\\', label=\\'Bad Variance %\\')\\naxes[1].axhline(0.2, color=\\'black\\', linestyle=\\'--\\', linewidth=1, label=\\'Jump Threshold (20%)\\')\\naxes[1].set_ylabel(\\'Bad Variance %\\')\\naxes[1].set_title(f\\'{sample_ticker}: Jump Component (Bad Variance)\\', fontsize=12, fontweight=\\'bold\\')\\naxes[1].legend(loc=\\'upper left\\')\\naxes[1].grid(True, alpha=0.3)\\n\\n# Plot 3: Relative volatility\\naxes[2].plot(sample_data[\\'Date\\'], sample_data[\\'RV_vs_market\\'], linewidth=1, color=\\'purple\\', alpha=0.7)\\naxes[2].axhline(1.0, color=\\'black\\', linestyle=\\'--\\', linewidth=1, label=\\'Market Average\\')\\naxes[2].fill_between(sample_data[\\'Date\\'], 1.0, sample_data[\\'RV_vs_market\\'], \\n                     where=(sample_data[\\'RV_vs_market\\'] >= 1.0), alpha=0.3, color=\\'red\\', label=\\'Above Market\\')\\naxes[2].fill_between(sample_data[\\'Date\\'], 1.0, sample_data[\\'RV_vs_market\\'], \\n                     where=(sample_data[\\'RV_vs_market\\'] < 1.0), alpha=0.3, color=\\'green\\', label=\\'Below Market\\')\\naxes[2].set_ylabel(\\'RV vs Market\\')\\naxes[2].set_title(f\\'{sample_ticker}: Relative Volatility vs Market\\', fontsize=12, fontweight=\\'bold\\')\\naxes[2].legend(loc=\\'upper left\\')\\naxes[2].grid(True, alpha=0.3)\\n\\n# Plot 4: Volatility rank\\naxes[3].plot(sample_data[\\'Date\\'], sample_data[\\'RV_rank\\'], linewidth=1, color=\\'darkorange\\', alpha=0.7)\\naxes[3].axhline(0.5, color=\\'black\\', linestyle=\\'--\\', linewidth=1, label=\\'Median Rank\\')\\naxes[3].fill_between(sample_data[\\'Date\\'], 0, 1, alpha=0.1, color=\\'gray\\')\\naxes[3].set_ylabel(\\'Volatility Rank (Percentile)\\')\\naxes[3].set_xlabel(\\'Date\\')\\naxes[3].set_title(f\\'{sample_ticker}: Volatility Rank Among All Stocks\\', fontsize=12, fontweight=\\'bold\\')\\naxes[3].legend(loc=\\'upper left\\')\\naxes[3].grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.savefig(\\'data/engineered/temporal_features_sample.png\\', dpi=150, bbox_inches=\\'tight\\')\\nplt.show()\\nprint(f\"  ✓ Temporal features plot saved: temporal_features_sample.png ({sample_ticker})\")\\n\\n# %% [markdown]\\n# ## 18. Feature Engineering Summary Report\\n# \\n# **Generate a comprehensive summary report.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FEATURE ENGINEERING SUMMARY REPORT\")\\nprint(\"=\"*70)\\n\\nsummary_report = f\"\"\"\\n{\\'=\\'*70}\\nFEATURE ENGINEERING SUMMARY REPORT\\nGenerated: {pd.Timestamp.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')}\\n{\\'=\\'*70}\\n\\nDATA OVERVIEW\\n-------------\\nSource File: RV_March2024.xlsx\\nDate Range: {df_master[\\'Date\\'].min().date()} to {df_master[\\'Date\\'].max().date()}\\nTrading Days: {df_master[\\'Date\\'].nunique()}\\nCompanies: {df_master[\\'Ticker\\'].nunique()}\\nTotal Observations: {len(df_master):,}\\n\\nFEATURE CATEGORIES\\n------------------\\n\"\"\"\\n\\nfor category, features in feature_categories.items():\\n    summary_report += f\"\\n{category}: {len(features)} features\"\\n\\nsummary_report += f\"\"\"\\n\\nTOTAL FEATURES: {total_features}\\n\\nDATA SPLITS\\n-----------\\nTraining Set: {len(train_data):,} obs ({len(train_data)/len(df_master)*100:.1f}%)\\n  Period: {train_data[\\'Date\\'].min().date()} to {train_data[\\'Date\\'].max().date()}\\n\\nValidation Set: {len(val_data):,} obs ({len(val_data)/len(df_master)*100:.1f}%)\\n  Period: {val_data[\\'Date\\'].min().date()} to {val_data[\\'Date\\'].max().date()}\\n\\nTest Set: {len(test_data):,} obs ({len(test_data)/len(df_master)*100:.1f}%)\\n  Period: {test_data[\\'Date\\'].min().date()} to {test_data[\\'Date\\'].max().date()}\\n\\nDATA QUALITY\\n------------\\nMissing Data (original): 4.24% (handled via imputation)\\nMissing Data (post-processing): {df_master.isna().sum().sum() / (len(df_master) * len(df_master.columns)) * 100:.2f}%\\nInfinite Values: 0 (replaced with NaN)\\nVariance Decomposition Error: {decomp_error.mean():.6f} (mean absolute)\\n\\nKEY ENGINEERED FEATURES\\n-----------------------\\nTemporal Features:\\n  - Lags: 1, 5, 10, 20 days\\n  - Rolling statistics: 5, 20, 60-day windows (mean, std, min, max)\\n  - Momentum indicators: 5-day, 20-day\\n\\nDecomposition Features:\\n  - Good/Bad variance ratios\\n  - Jump indicators (threshold: 20%)\\n  - Jump frequency and intensity\\n  - Cross-frequency comparisons\\n\\nCross-Sectional Features:\\n  - Market-wide statistics (mean, median, std, min, max)\\n  - Relative volatility vs market\\n  - Volatility z-scores and rankings\\n  - Market dispersion measures\\n\\nFrequency Features:\\n  - 1-min vs 5-min ratios\\n  - Microstructure noise estimates\\n  - Frequency consistency measures\\n\\nCalendar Features:\\n  - Year, month, quarter, day of week\\n  - Monday/Friday indicators\\n  - Month-end effects\\n\\nData Quality Features:\\n  - Missingness indicators\\n  - Data completeness scores\\n  - Consecutive missing days\\n\\nOUTPUT FILES\\n------------\\nLocation: data/engineered/\\n\\nDatasets:\\n  ✓ rv_features_full.parquet (complete dataset)\\n  ✓ rv_features_full.csv (complete dataset)\\n  ✓ rv_features_train.parquet (training set)\\n  ✓ rv_features_val.parquet (validation set)\\n  ✓ rv_features_test.parquet (test set)\\n\\nMetadata:\\n  ✓ feature_metadata.csv (feature details)\\n  ✓ feature_categories.csv (category mapping)\\n\\nVisualizations:\\n  ✓ feature_correlations.png\\n  ✓ feature_distributions.png\\n  ✓ temporal_features_sample.png\\n\\nNEXT STEPS\\n----------\\n1. Load engineered data: pd.read_parquet(\\'data/engineered/rv_features_train.parquet\\')\\n2. Select features for modeling\\n3. Apply feature scaling/normalization as needed\\n4. Build and train predictive models\\n5. Evaluate on validation set\\n6. Final evaluation on held-out test set\\n\\nNOTES\\n-----\\n- Time-based split ensures no data leakage\\n- All temporal features respect chronological order\\n- Missing data handled systematically (forward fill + interpolation)\\n- Features ready for machine learning without further preprocessing\\n\\n{\\'=\\'*70}\\nEND OF REPORT\\n{\\'=\\'*70}\\n\"\"\"\\n\\nprint(summary_report)\\n\\n# Save report to file\\nwith open(\\'data/engineered/feature_engineering_report.txt\\', \\'w\\') as f:\\n    f.write(summary_report)\\nprint(\"\\n  ✓ Summary report saved: feature_engineering_report.txt\")\\n\\n# %% [markdown]\\n# ## 19. Quick Data Loading Template (for Future Use)\\n# \\n# **Template code for loading engineered data in modeling notebooks.**\\n\\n# %%\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"DATA LOADING TEMPLATE FOR MODELING\")\\nprint(\"=\"*70)\\n\\ntemplate_code = \"\"\"\\n# ============================================\\n# TEMPLATE: Load Engineered Features\\n# ============================================\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load training data\\ntrain_df = pd.read_parquet(\\'data/engineered/rv_features_train.parquet\\')\\n\\n# Load validation data\\nval_df = pd.read_parquet(\\'data/engineered/rv_features_val.parquet\\')\\n\\n# Load test data\\ntest_df = pd.read_parquet(\\'data/engineered/rv_features_test.parquet\\')\\n\\n# Load feature metadata\\nfeature_metadata = pd.read_csv(\\'data/engineered/feature_metadata.csv\\')\\n\\nprint(f\"Training samples: {len(train_df):,}\")\\nprint(f\"Validation samples: {len(val_df):,}\")\\nprint(f\"Test samples: {len(test_df):,}\")\\nprint(f\"Total features: {len(train_df.columns)}\")\\n\\n# Define feature groups\\nidentifier_cols = [\\'Date\\', \\'Ticker\\']\\ntarget_col = \\'RV\\'\\nfeature_cols = [c for c in train_df.columns if c not in identifier_cols + [target_col]]\\n\\n# Prepare X and y\\nX_train = train_df[feature_cols]\\ny_train = train_df[target_col]\\n\\nX_val = val_df[feature_cols]\\ny_val = val_df[target_col]\\n\\nX_test = test_df[feature_cols]\\ny_test = test_df[target_col]\\n\\nprint(f\"\\\\nFeature matrix shape: {X_train.shape}\")\\nprint(f\"Target vector shape: {y_train.shape}\")\\n\\n# Optional: Select specific feature categories\\ntemporal_features = [c for c in feature_cols if \\'lag\\' in c or \\'roll\\' in c]\\ndecomp_features = [c for c in feature_cols if \\'Good\\' in c or \\'Bad\\' in c or \\'jump\\' in c]\\nmarket_features = [c for c in feature_cols if \\'market\\' in c or \\'rank\\' in c]\\n\\n# Ready for modeling!\\n\"\"\"\\n\\nprint(template_code)\\n\\n# Save template\\nwith open(\\'data/engineered/loading_template.py\\', \\'w\\') as f:\\n    f.write(template_code)\\nprint(\"  ✓ Loading template saved: loading_template.py\")\\n\\n# %% [markdown]\\n# ## Conclusion\\n# \\n# **Feature engineering complete!** \\n# \\n# We have successfully transformed raw high-frequency volatility data into a rich, modeling-ready dataset with:\\n# - ✅ **{total_features} engineered features** across 8 categories\\n# - ✅ **{len(df_master):,} observations** from 30 Dow Jones stocks\\n# - ✅ **Time-based splits** (train/val/test) for proper evaluation\\n# - ✅ **Clean data** with systematic missing data handling\\n# - ✅ **Comprehensive documentation** and metadata\\n# \\n# **Key Highlights:**\\n# 1. Temporal features capture volatility dynamics and persistence\\n# 2. Decomposition features enable jump detection and risk analysis\\n# 3. Cross-sectional features provide market context\\n# 4. Calendar features capture seasonal patterns\\n# 5. Data quality features track imputation and completeness\\n# \\n# **Ready for:** Time series forecasting, classification (jump detection), clustering, anomaly detection, and risk modeling.\\n# \\n# **Next Notebook:** `3_Model_Development.ipynb`\\n\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FEATURE ENGINEERING PIPELINE COMPLETE!\")\\nprint(\"=\"*70)\\nprint(f\"\\n✓ Total features engineered: {total_features}\")\\nprint(f\"✓ Final dataset size: {len(df_master):,} observations\")\\nprint(f\"✓ Ready for modeling: train/val/test splits created\")\\nprint(f\"✓ All outputs saved to: data/engineered/\")\\nprint(f\"\\nNext step: Model Development (3_Model_Development.ipynb)\")\\nprint(\"\\n\" + \"=\"*70)\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load reference sheets\n",
    "dates_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Dates\", header=None)\n",
    "companies_df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=\"Companies\", header=None)\n",
    "\n",
    "# Get lists for iteration\n",
    "dates_list = pd.to_datetime(dates_df[0], format='%d-%b-%Y')\n",
    "companies_list = companies_df[0].tolist()\n",
    "\n",
    "print(f\"Data Period: {dates_list.min().date()} to {dates_list.max().date()}\")\n",
    "print(f\"Number of trading days: {len(dates_list)}\")\n",
    "print(f\"Number of companies: {len(companies_list)}\")\n",
    "print(f\"\\nCompanies: {', '.join(companies_list)}\")\n",
    "\n",
    "# Load all volatility measure sheets\n",
    "sheet_names = ['RV', 'BPV', 'Good', 'Bad', 'RQ', 'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5']\n",
    "data_dict = {}\n",
    "\n",
    "print(f\"\\nLoading {len(sheet_names)} data sheets...\")\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(\"RV_March2024.xlsx\", sheet_name=sheet, header=None)\n",
    "    df.columns = companies_list\n",
    "    df.index = dates_list\n",
    "    df.index.name = 'Date'\n",
    "    data_dict[sheet] = df\n",
    "    print(f\"  ✓ {sheet:8s}: {df.shape}\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Preprocessing and Cleaning\n",
    "# \n",
    "# **Key Steps:**\n",
    "# - Replace zeros with NaN (missing data encoding)\n",
    "# - Create a master dataset combining all measures\n",
    "# - Handle missing data systematically\n",
    "# - Validate data integrity\n",
    "\n",
    "# %%\n",
    "# Replace zeros with NaN across all sheets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sheet_name, df in data_dict.items():\n",
    "    data_dict[sheet_name] = df.replace(0, np.nan)\n",
    "    missing_pct = (df.isna().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"{sheet_name:8s}: {missing_pct:.2f}% missing data\")\n",
    "\n",
    "# Create long-format master dataset\n",
    "print(\"\\nCreating master dataset in long format...\")\n",
    "\n",
    "master_data = []\n",
    "for ticker in companies_list:\n",
    "    ticker_data = pd.DataFrame({\n",
    "        'Date': dates_list,\n",
    "        'Ticker': ticker\n",
    "    })\n",
    "    \n",
    "    # Add all measures for this ticker\n",
    "    for sheet_name in sheet_names:\n",
    "        ticker_data[sheet_name] = data_dict[sheet_name][ticker].values\n",
    "    \n",
    "    master_data.append(ticker_data)\n",
    "\n",
    "df_master = pd.concat(master_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\nMaster dataset created: {df_master.shape}\")\n",
    "print(f\"Total observations: {len(df_master):,}\")\n",
    "print(f\"Features: Date, Ticker + {len(sheet_names)} volatility measures\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of master dataset:\")\n",
    "print(df_master.head(10))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Feature Creation - Temporal Features\n",
    "# \n",
    "# **Temporal features capture time-series dynamics:**\n",
    "# - **Lags**: Past values (t-1, t-5, t-20)\n",
    "# - **Rolling statistics**: Moving averages, standard deviations\n",
    "# - **Momentum**: Rate of change indicators\n",
    "# - **Trend**: Direction and strength of volatility trends\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TEMPORAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by ticker and date for proper time series operations\n",
    "df_master = df_master.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Define lag periods\n",
    "lag_periods = [1, 5, 10, 20]  # 1 day, 1 week, 2 weeks, 1 month\n",
    "\n",
    "# Define rolling windows\n",
    "rolling_windows = [5, 20, 60]  # 1 week, 1 month, 3 months\n",
    "\n",
    "# Create lag features for RV (primary target)\n",
    "print(\"\\nCreating lag features for RV...\")\n",
    "for lag in lag_periods:\n",
    "    df_master[f'RV_lag{lag}'] = df_master.groupby('Ticker')['RV'].shift(lag)\n",
    "    print(f\"  ✓ RV_lag{lag}\")\n",
    "\n",
    "# Create rolling statistics\n",
    "print(\"\\nCreating rolling statistics...\")\n",
    "for window in rolling_windows:\n",
    "    # Rolling mean\n",
    "    df_master[f'RV_roll_mean_{window}'] = (\n",
    "        df_master.groupby('Ticker')['RV']\n",
    "        .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    )\n",
    "    \n",
    "    # Rolling std\n",
    "    df_master[f'RV_roll_std_{window}'] = (\n",
    "        df_master.groupby('Ticker')['RV']\n",
    "        .transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "    )\n",
    "    \n",
    "    # Rolling min/max\n",
    "    df_master[f'RV_roll_min_{window}'] = (\n",
    "        df_master.groupby('Ticker')['RV']\n",
    "        .transform(lambda x: x.rolling(window=window, min_periods=1).min())\n",
    "    )\n",
    "    \n",
    "    df_master[f'RV_roll_max_{window}'] = (\n",
    "        df_master.groupby('Ticker')['RV']\n",
    "        .transform(lambda x: x.rolling(window=window, min_periods=1).max())\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Rolling features (window={window})\")\n",
    "\n",
    "# Momentum features\n",
    "print(\"\\nCreating momentum features...\")\n",
    "df_master['RV_momentum_5'] = df_master['RV'] / df_master['RV_lag5'] - 1\n",
    "df_master['RV_momentum_20'] = df_master['RV'] / df_master['RV_lag20'] - 1\n",
    "\n",
    "# Volatility of volatility\n",
    "df_master['RV_volatility_20'] = (\n",
    "    df_master.groupby('Ticker')['RV']\n",
    "    .transform(lambda x: x.rolling(window=20, min_periods=1).std())\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal temporal features created: {len([c for c in df_master.columns if 'lag' in c or 'roll' in c or 'momentum' in c])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Feature Creation - Variance Decomposition Features\n",
    "# \n",
    "# **Decomposition features leverage the Good/Bad variance split:**\n",
    "# - **Jump indicators**: Bad variance relative to total\n",
    "# - **Continuous component strength**: Good variance ratios\n",
    "# - **Jump detection signals**: Threshold-based indicators\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VARIANCE DECOMPOSITION FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Good/Bad variance ratios\n",
    "print(\"\\nCreating variance decomposition ratios...\")\n",
    "\n",
    "df_master['Good_Bad_ratio'] = df_master['Good'] / df_master['Bad']\n",
    "df_master['Bad_pct'] = df_master['Bad'] / df_master['RV']\n",
    "df_master['Good_pct'] = df_master['Good'] / df_master['RV']\n",
    "\n",
    "# Verify decomposition (should be close to 1.0)\n",
    "df_master['decomp_check'] = (df_master['Good'] + df_master['Bad']) / df_master['RV']\n",
    "\n",
    "print(\"  ✓ Good/Bad ratios\")\n",
    "print(\"  ✓ Bad percentage (jump indicator)\")\n",
    "print(\"  ✓ Good percentage (continuous component)\")\n",
    "\n",
    "# Jump detection features\n",
    "print(\"\\nCreating jump detection features...\")\n",
    "\n",
    "# Define jump threshold (Bad variance > 20% of total)\n",
    "JUMP_THRESHOLD = 0.20\n",
    "\n",
    "df_master['jump_indicator'] = (df_master['Bad_pct'] > JUMP_THRESHOLD).astype(int)\n",
    "\n",
    "# Rolling jump frequency\n",
    "df_master['jump_freq_20'] = (\n",
    "    df_master.groupby('Ticker')['jump_indicator']\n",
    "    .transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# Jump intensity\n",
    "df_master['jump_intensity'] = df_master['Bad'] * df_master['jump_indicator']\n",
    "\n",
    "print(f\"  ✓ Jump indicator (threshold={JUMP_THRESHOLD})\")\n",
    "print(f\"  ✓ Rolling jump frequency\")\n",
    "print(f\"  ✓ Jump intensity\")\n",
    "\n",
    "# Compare 1-min vs 5-min decomposition\n",
    "print(\"\\nCreating cross-frequency decomposition features...\")\n",
    "\n",
    "df_master['Bad_pct_5'] = df_master['Bad_5'] / df_master['RV_5']\n",
    "df_master['jump_diff_freq'] = df_master['Bad_pct'] - df_master['Bad_pct_5']\n",
    "\n",
    "print(\"  ✓ Cross-frequency jump comparison\")\n",
    "\n",
    "print(f\"\\nTotal decomposition features: {len([c for c in df_master.columns if 'Good' in c or 'Bad' in c or 'jump' in c])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Feature Creation - Cross-Sectional Features\n",
    "# \n",
    "# **Cross-sectional features compare a stock's volatility to the market:**\n",
    "# - **Relative volatility**: Stock RV vs market average\n",
    "# - **Volatility rank**: Percentile ranking among stocks\n",
    "# - **Market-wide measures**: Average, median, spread across all stocks\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING CROSS-SECTIONAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate market-wide statistics for each date\n",
    "print(\"\\nCalculating market-wide statistics...\")\n",
    "\n",
    "market_stats = df_master.groupby('Date').agg({\n",
    "    'RV': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'Bad_pct': ['mean', 'median'],\n",
    "    'jump_indicator': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "market_stats.columns = ['Date', 'market_RV_mean', 'market_RV_median', \n",
    "                        'market_RV_std', 'market_RV_min', 'market_RV_max',\n",
    "                        'market_Bad_pct_mean', 'market_Bad_pct_median',\n",
    "                        'market_jump_count']\n",
    "\n",
    "# Merge back to main dataset\n",
    "df_master = df_master.merge(market_stats, on='Date', how='left')\n",
    "\n",
    "print(\"  ✓ Market-wide RV statistics\")\n",
    "print(\"  ✓ Market-wide jump statistics\")\n",
    "\n",
    "# Relative volatility features\n",
    "print(\"\\nCreating relative volatility features...\")\n",
    "\n",
    "df_master['RV_vs_market'] = df_master['RV'] / df_master['market_RV_mean']\n",
    "df_master['RV_zscore'] = ((df_master['RV'] - df_master['market_RV_mean']) / \n",
    "                          df_master['market_RV_std'])\n",
    "\n",
    "# Volatility rank (percentile)\n",
    "df_master['RV_rank'] = (\n",
    "    df_master.groupby('Date')['RV']\n",
    "    .rank(pct=True)\n",
    ")\n",
    "\n",
    "print(\"  ✓ Relative volatility vs market\")\n",
    "print(\"  ✓ Volatility z-score\")\n",
    "print(\"  ✓ Volatility rank (percentile)\")\n",
    "\n",
    "# Market dispersion\n",
    "df_master['market_dispersion'] = df_master['market_RV_max'] - df_master['market_RV_min']\n",
    "df_master['market_CV'] = df_master['market_RV_std'] / df_master['market_RV_mean']\n",
    "\n",
    "print(\"  ✓ Market dispersion measures\")\n",
    "\n",
    "print(f\"\\nTotal cross-sectional features: {len([c for c in df_master.columns if 'market' in c or 'rank' in c or 'vs_market' in c])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Feature Creation - Frequency Relationship Features\n",
    "# \n",
    "# **Leverage the relationship between 1-min and 5-min measures:**\n",
    "# - **Microstructure noise**: 1-min vs 5-min differences\n",
    "# - **Frequency ratios**: How measures scale across frequencies\n",
    "# - **Consistency indicators**: Agreement between frequencies\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FREQUENCY RELATIONSHIP FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1-min vs 5-min ratios\n",
    "print(\"\\nCreating frequency ratio features...\")\n",
    "\n",
    "df_master['RV_freq_ratio'] = df_master['RV'] / df_master['RV_5']\n",
    "df_master['BPV_freq_ratio'] = df_master['BPV'] / df_master['BPV_5']\n",
    "df_master['Good_freq_ratio'] = df_master['Good'] / df_master['Good_5']\n",
    "df_master['Bad_freq_ratio'] = df_master['Bad'] / df_master['Bad_5']\n",
    "\n",
    "print(\"  ✓ RV frequency ratio (1-min/5-min)\")\n",
    "print(\"  ✓ BPV frequency ratio\")\n",
    "print(\"  ✓ Good/Bad frequency ratios\")\n",
    "\n",
    "# Microstructure noise indicator\n",
    "# Theory: excess RV at 1-min suggests noise\n",
    "df_master['microstructure_noise'] = df_master['RV'] - df_master['RV_5']\n",
    "\n",
    "# Frequency consistency\n",
    "df_master['freq_consistency'] = np.abs(1 - df_master['RV_freq_ratio'])\n",
    "\n",
    "print(\"  ✓ Microstructure noise estimate\")\n",
    "print(\"  ✓ Frequency consistency measure\")\n",
    "\n",
    "print(f\"\\nTotal frequency features: {len([c for c in df_master.columns if 'freq' in c or 'microstructure' in c])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Feature Creation - Calendar and Time Features\n",
    "# \n",
    "# **Time-based features can capture seasonality and patterns:**\n",
    "# - **Day of week**: Monday effect, Friday effect\n",
    "# - **Month**: Seasonal patterns\n",
    "# - **Quarter**: Quarterly earnings effects\n",
    "# - **Year**: Long-term trends\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING CALENDAR FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract time components\n",
    "df_master['year'] = df_master['Date'].dt.year\n",
    "df_master['month'] = df_master['Date'].dt.month\n",
    "df_master['quarter'] = df_master['Date'].dt.quarter\n",
    "df_master['day_of_week'] = df_master['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df_master['day_of_month'] = df_master['Date'].dt.day\n",
    "df_master['week_of_year'] = df_master['Date'].dt.isocalendar().week\n",
    "\n",
    "print(\"  ✓ Year, Month, Quarter\")\n",
    "print(\"  ✓ Day of week, Day of month\")\n",
    "print(\"  ✓ Week of year\")\n",
    "\n",
    "# Is Monday/Friday (known volatility patterns)\n",
    "df_master['is_monday'] = (df_master['day_of_week'] == 0).astype(int)\n",
    "df_master['is_friday'] = (df_master['day_of_week'] == 4).astype(int)\n",
    "\n",
    "# Month-end effect (last 3 days of month)\n",
    "df_master['is_month_end'] = (df_master['day_of_month'] >= 28).astype(int)\n",
    "\n",
    "print(\"  ✓ Monday/Friday indicators\")\n",
    "print(\"  ✓ Month-end indicator\")\n",
    "\n",
    "print(f\"\\nTotal calendar features: {len([c for c in df_master.columns if c in ['year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year', 'is_monday', 'is_friday', 'is_month_end']])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Feature Creation - Data Quality Features\n",
    "# \n",
    "# **Track data availability and quality:**\n",
    "# - **Missing data indicators**: Whether original data was missing\n",
    "# - **Data completeness scores**: How much recent data is available\n",
    "# - **Imputation flags**: Mark imputed values\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DATA QUALITY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create missingness indicators (before any imputation)\n",
    "print(\"\\nCreating missingness indicators...\")\n",
    "\n",
    "df_master['RV_is_missing'] = df_master['RV'].isna().astype(int)\n",
    "df_master['BPV_is_missing'] = df_master['BPV'].isna().astype(int)\n",
    "df_master['Good_is_missing'] = df_master['Good'].isna().astype(int)\n",
    "df_master['Bad_is_missing'] = df_master['Bad'].isna().astype(int)\n",
    "\n",
    "# Data completeness score (% of non-missing in last 20 days)\n",
    "df_master['data_completeness_20'] = (\n",
    "    df_master.groupby('Ticker')['RV_is_missing']\n",
    "    .transform(lambda x: 1 - x.rolling(window=20, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "print(\"  ✓ Missingness indicators for key measures\")\n",
    "print(\"  ✓ Data completeness score (20-day)\")\n",
    "\n",
    "# Consecutive missing days\n",
    "df_master['consec_missing'] = (\n",
    "    df_master.groupby('Ticker')['RV_is_missing']\n",
    "    .transform(lambda x: x.groupby((x != x.shift()).cumsum()).cumsum())\n",
    ")\n",
    "\n",
    "print(\"  ✓ Consecutive missing days counter\")\n",
    "\n",
    "print(f\"\\nTotal data quality features: {len([c for c in df_master.columns if 'missing' in c or 'completeness' in c])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Handle Missing Data\n",
    "# \n",
    "# **Strategy:**\n",
    "# 1. **Forward fill**: For short gaps (≤3 days)\n",
    "# 2. **Linear interpolation**: For medium gaps (4-10 days)\n",
    "# 3. **Leave as NaN**: For long gaps (>10 days) - will drop these rows for modeling\n",
    "# \n",
    "# Note: We've already created missingness indicators, so we preserve information about what was imputed.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count missing before imputation\n",
    "missing_before = df_master['RV'].isna().sum()\n",
    "print(f\"\\nMissing RV values before imputation: {missing_before:,} ({missing_before/len(df_master)*100:.2f}%)\")\n",
    "\n",
    "# Apply imputation strategy by ticker\n",
    "print(\"\\nApplying imputation strategy...\")\n",
    "\n",
    "imputation_cols = ['RV', 'BPV', 'Good', 'Bad', 'RQ', 'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5']\n",
    "\n",
    "for ticker in companies_list:\n",
    "    ticker_mask = df_master['Ticker'] == ticker\n",
    "    \n",
    "    for col in imputation_cols:\n",
    "        # Forward fill (limit to 3 periods)\n",
    "        df_master.loc[ticker_mask, col] = (\n",
    "            df_master.loc[ticker_mask, col].fillna(method='ffill', limit=3)\n",
    "        )\n",
    "        \n",
    "        # Interpolate remaining gaps (limit to 10 periods)\n",
    "        df_master.loc[ticker_mask, col] = (\n",
    "            df_master.loc[ticker_mask, col].interpolate(method='linear', limit=10)\n",
    "        )\n",
    "\n",
    "missing_after = df_master['RV'].isna().sum()\n",
    "print(f\"Missing RV values after imputation: {missing_after:,} ({missing_after/len(df_master)*100:.2f}%)\")\n",
    "print(f\"Imputation recovered: {missing_before - missing_after:,} values ({(missing_before-missing_after)/missing_before*100:.2f}%)\")\n",
    "\n",
    "# Drop rows with remaining missing values in key columns\n",
    "print(\"\\nDropping rows with remaining missing values in key measures...\")\n",
    "rows_before = len(df_master)\n",
    "df_master = df_master.dropna(subset=['RV', 'Good', 'Bad'])\n",
    "rows_after = len(df_master)\n",
    "print(f\"Rows dropped: {rows_before - rows_after:,} ({(rows_before-rows_after)/rows_before*100:.2f}%)\")\n",
    "print(f\"Final dataset size: {rows_after:,} observations\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Feature Summary and Validation\n",
    "# \n",
    "# **Review all created features and validate data integrity.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Categorize features\n",
    "feature_categories = {\n",
    "    'Identifier': ['Date', 'Ticker'],\n",
    "    'Original Measures': sheet_names,\n",
    "    'Temporal': [c for c in df_master.columns if 'lag' in c or 'roll' in c or 'momentum' in c or 'volatility' in c],\n",
    "    'Decomposition': [c for c in df_master.columns if ('Good' in c or 'Bad' in c or 'jump' in c or 'decomp' in c) and c not in sheet_names],\n",
    "    'Cross-Sectional': [c for c in df_master.columns if 'market' in c or 'rank' in c or 'vs_market' in c or 'dispersion' in c or 'CV' in c],\n",
    "    'Frequency': [c for c in df_master.columns if 'freq' in c or 'microstructure' in c],\n",
    "    'Calendar': ['year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year', 'is_monday', 'is_friday', 'is_month_end'],\n",
    "    'Data Quality': [c for c in df_master.columns if 'missing' in c or 'completeness' in c or 'consec' in c]\n",
    "}\n",
    "\n",
    "total_features = 0\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"  Count: {len(features)}\")\n",
    "    if len(features) <= 10:\n",
    "        print(f\"  Features: {', '.join(features)}\")\n",
    "    else:\n",
    "        print(f\"  Features: {', '.join(features[:5])} ... (showing 5/{len(features)})\")\n",
    "    total_features += len(features)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOTAL FEATURES: {total_features}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(\"\\nFinal Dataset Structure:\")\n",
    "print(df_master.info(verbose=False))\n",
    "\n",
    "print(\"\\nSample of engineered features:\")\n",
    "sample_cols = ['Date', 'Ticker', 'RV', 'RV_lag1', 'RV_roll_mean_20', \n",
    "               'Bad_pct', 'jump_indicator', 'RV_vs_market', 'RV_rank']\n",
    "print(df_master[sample_cols].head(10))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Data Validation and Quality Checks\n",
    "# \n",
    "# **Verify data integrity before saving for modeling.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for infinite values\n",
    "print(\"\\nChecking for infinite values...\")\n",
    "inf_cols = df_master.columns[df_master.isin([np.inf, -np.inf]).any()].tolist()\n",
    "if inf_cols:\n",
    "    print(f\"  ⚠️  Found infinite values in: {', '.join(inf_cols)}\")\n",
    "    # Replace infinite values with NaN\n",
    "    df_master.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(\"  ✓ Infinite values replaced with NaN\")\n",
    "else:\n",
    "    print(\"  ✓ No infinite values found\")\n",
    "\n",
    "# Check for remaining missing values\n",
    "print(\"\\nChecking for missing values...\")\n",
    "missing_summary = df_master.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"  Columns with missing values: {len(missing_summary)}\")\n",
    "    print(missing_summary.head(10))\n",
    "else:\n",
    "    print(\"  ✓ No missing values in core features\")\n",
    "\n",
    "# Validate variance decomposition\n",
    "print(\"\\nValidating variance decomposition...\")\n",
    "decomp_error = (df_master['Good'] + df_master['Bad'] - df_master['RV']).abs()\n",
    "print(f\"  Mean absolute error: {decomp_error.mean():.6f}\")\n",
    "print(f\"  Max absolute error: {decomp_error.max():.6f}\")\n",
    "if decomp_error.mean() < 0.01:\n",
    "    print(\"  ✓ Decomposition validation passed\")\n",
    "else:\n",
    "    print(\"  ⚠️  Decomposition has larger than expected errors\")\n",
    "\n",
    "# Validate date continuity\n",
    "print(\"\\nValidating temporal continuity...\")\n",
    "for ticker in companies_list[:3]:  # Check first 3 tickers\n",
    "    ticker_dates = df_master[df_master['Ticker'] == ticker]['Date'].sort_values()\n",
    "    date_gaps = ticker_dates.diff().dt.days.dropna()\n",
    "    max_gap = date_gaps.max()\n",
    "    if max_gap > 5:  # More than 5 days suggests missing dates\n",
    "        print(f\"  ⚠️  {ticker}: Max gap = {max_gap} days\")\n",
    "print(\"  ✓ Temporal continuity check complete\")\n",
    "\n",
    "# Check feature distributions\n",
    "print(\"\\nFeature distribution summary (RV):\")\n",
    "print(df_master['RV'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Train/Validation/Test Split\n",
    "# \n",
    "# **Create time-based splits for modeling:**\n",
    "# - **Training**: 2003-2018 (60%)\n",
    "# - **Validation**: 2019-2021 (20%)\n",
    "# - **Test**: 2022-2024 (20%)\n",
    "# \n",
    "# Time-based split prevents data leakage and simulates real-world forecasting.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TRAIN/VALIDATION/TEST SPLITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define split dates\n",
    "train_end = '2018-12-31'\n",
    "val_end = '2021-12-31'\n",
    "\n",
    "# Create splits\n",
    "train_data = df_master[df_master['Date'] <= train_end].copy()\n",
    "val_data = df_master[(df_master['Date'] > train_end) & (df_master['Date'] <= val_end)].copy()\n",
    "test_data = df_master[df_master['Date'] > val_end].copy()\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Date range: {train_data['Date'].min().date()} to {train_data['Date'].max().date()}\")\n",
    "print(f\"  Observations: {len(train_data):,}\")\n",
    "print(f\"  Companies: {train_data['Ticker'].nunique()}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Date range: {val_data['Date'].min().date()} to {val_data['Date'].max().date()}\")\n",
    "print(f\"  Observations: {len(val_data):,}\")\n",
    "print(f\"  Companies: {val_data['Ticker'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Date range: {test_data['Date'].min().date()} to {test_data['Date'].max().date()}\")\n",
    "print(f\"  Observations: {len(test_data):,}\")\n",
    "print(f\"  Companies: {test_data['Ticker'].nunique()}\")\n",
    "\n",
    "# Visualize split\n",
    "print(\"\\nDistribution across splits:\")\n",
    "split_dist = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'Count': [len(train_data), len(val_data), len(test_data)],\n",
    "    'Percentage': [len(train_data)/len(df_master)*100, \n",
    "                   len(val_data)/len(df_master)*100,\n",
    "                   len(test_data)/len(df_master)*100]\n",
    "})\n",
    "print(split_dist)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Save Engineered Features\n",
    "# \n",
    "# **Save the feature-engineered dataset for modeling:**\n",
    "# - Full dataset (all features)\n",
    "# - Train/validation/test splits\n",
    "# - Feature metadata\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING ENGINEERED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('data/engineered', exist_ok=True)\n",
    "\n",
    "# Save full dataset\n",
    "print(\"\\nSaving full engineered dataset...\")\n",
    "df_master.to_parquet('data/engineered/rv_features_full.parquet', index=False)\n",
    "df_master.to_csv('data/engineered/rv_features_full.csv', index=False)\n",
    "print(\"  ✓ rv_features_full.parquet\")\n",
    "print(\"  ✓ rv_features_full.csv\")\n",
    "\n",
    "# Save splits\n",
    "print(\"\\nSaving train/val/test splits...\")\n",
    "train_data.to_parquet('data/engineered/rv_features_train.parquet', index=False)\n",
    "val_data.to_parquet('data/engineered/rv_features_val.parquet', index=False)\n",
    "test_data.to_parquet('data/engineered/rv_features_test.parquet', index=False)\n",
    "print(\"  ✓ rv_features_train.parquet\")\n",
    "print(\"  ✓ rv_features_val.parquet\")\n",
    "print(\"  ✓ rv_features_test.parquet\")\n",
    "\n",
    "# Save feature metadata\n",
    "print(\"\\nSaving feature metadata...\")\n",
    "feature_metadata = pd.DataFrame({\n",
    "    'Feature': df_master.columns,\n",
    "    'Dtype': df_master.dtypes.values,\n",
    "    'Missing_Count': df_master.isna().sum().values,\n",
    "    'Missing_Pct': (df_master.isna().sum().values / len(df_master) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Add category information\n",
    "feature_metadata['Category'] = 'Other'\n",
    "for category, features in feature_categories.items():\n",
    "    feature_metadata.loc[feature_metadata['Feature'].isin(features), 'Category'] = category\n",
    "\n",
    "feature_metadata.to_csv('data/engineered/feature_metadata.csv', index=False)\n",
    "print(\"  ✓ feature_metadata.csv\")\n",
    "\n",
    "# Save feature category mapping\n",
    "category_summary = pd.DataFrame([\n",
    "    {'Category': cat, 'Count': len(feats), 'Features': ', '.join(feats[:3]) + '...' if len(feats) > 3 else ', '.join(feats)}\n",
    "    for cat, feats in feature_categories.items()\n",
    "])\n",
    "category_summary.to_csv('data/engineered/feature_categories.csv', index=False)\n",
    "print(\"  ✓ feature_categories.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput location: data/engineered/\")\n",
    "print(f\"Files created: 8\")\n",
    "print(f\"  - Full dataset: parquet + csv\")\n",
    "print(f\"  - Train/val/test splits: 3 parquet files\")\n",
    "print(f\"  - Metadata: 2 csv files\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Feature Importance Analysis (Preview)\n",
    "# \n",
    "# **Quick analysis of feature correlations with target (RV):**\n",
    "# This helps us understand which features might be most predictive.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select numerical features (exclude identifiers and categorical)\n",
    "numerical_features = df_master.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year']\n",
    "numerical_features = [f for f in numerical_features if f not in exclude_cols]\n",
    "\n",
    "# Calculate correlation with RV (target)\n",
    "correlations = df_master[numerical_features].corr()['RV'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features Most Correlated with RV:\")\n",
    "print(correlations.head(20))\n",
    "\n",
    "print(\"\\nBottom 10 Features (Least Correlated with RV):\")\n",
    "print(correlations.tail(10))\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = correlations.head(21)[1:]  # Exclude RV itself\n",
    "top_features.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Correlation with RV')\n",
    "ax.set_title('Top 20 Features Correlated with Realized Variance (RV)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/engineered/feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n  ✓ Correlation plot saved: feature_correlations.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Feature Distribution Visualization\n",
    "# \n",
    "# **Visualize distributions of key engineered features.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE DISTRIBUTION VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select key features to visualize\n",
    "key_features = [\n",
    "    'RV', 'RV_lag1', 'RV_roll_mean_20', 'Bad_pct', \n",
    "    'jump_indicator', 'RV_vs_market', 'RV_rank', 'microstructure_noise'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df_master.columns:\n",
    "        data = df_master[feature].dropna()\n",
    "        \n",
    "        # Plot histogram\n",
    "        axes[idx].hist(data, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
    "        axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
    "        axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/engineered/feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"  ✓ Distribution plot saved: feature_distributions.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 17. Time Series Feature Visualization\n",
    "# \n",
    "# **Visualize temporal features for a sample ticker.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPORAL FEATURE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select a sample ticker\n",
    "sample_ticker = 'AAPL'\n",
    "sample_data = df_master[df_master['Ticker'] == sample_ticker].sort_values('Date')\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: RV with moving averages\n",
    "axes[0].plot(sample_data['Date'], sample_data['RV'], linewidth=0.8, alpha=0.7, label='RV', color='navy')\n",
    "axes[0].plot(sample_data['Date'], sample_data['RV_roll_mean_20'], linewidth=1.5, label='20-day MA', color='red')\n",
    "axes[0].plot(sample_data['Date'], sample_data['RV_roll_mean_60'], linewidth=1.5, label='60-day MA', color='green')\n",
    "axes[0].set_ylabel('Realized Variance')\n",
    "axes[0].set_title(f'{sample_ticker}: Realized Variance with Moving Averages', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Jump indicators\n",
    "axes[1].fill_between(sample_data['Date'], 0, sample_data['Bad_pct'], alpha=0.6, color='red', label='Bad Variance %')\n",
    "axes[1].axhline(0.2, color='black', linestyle='--', linewidth=1, label='Jump Threshold (20%)')\n",
    "axes[1].set_ylabel('Bad Variance %')\n",
    "axes[1].set_title(f'{sample_ticker}: Jump Component (Bad Variance)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Relative volatility\n",
    "axes[2].plot(sample_data['Date'], sample_data['RV_vs_market'], linewidth=1, color='purple', alpha=0.7)\n",
    "axes[2].axhline(1.0, color='black', linestyle='--', linewidth=1, label='Market Average')\n",
    "axes[2].fill_between(sample_data['Date'], 1.0, sample_data['RV_vs_market'], \n",
    "                     where=(sample_data['RV_vs_market'] >= 1.0), alpha=0.3, color='red', label='Above Market')\n",
    "axes[2].fill_between(sample_data['Date'], 1.0, sample_data['RV_vs_market'], \n",
    "                     where=(sample_data['RV_vs_market'] < 1.0), alpha=0.3, color='green', label='Below Market')\n",
    "axes[2].set_ylabel('RV vs Market')\n",
    "axes[2].set_title(f'{sample_ticker}: Relative Volatility vs Market', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Volatility rank\n",
    "axes[3].plot(sample_data['Date'], sample_data['RV_rank'], linewidth=1, color='darkorange', alpha=0.7)\n",
    "axes[3].axhline(0.5, color='black', linestyle='--', linewidth=1, label='Median Rank')\n",
    "axes[3].fill_between(sample_data['Date'], 0, 1, alpha=0.1, color='gray')\n",
    "axes[3].set_ylabel('Volatility Rank (Percentile)')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_title(f'{sample_ticker}: Volatility Rank Among All Stocks', fontsize=12, fontweight='bold')\n",
    "axes[3].legend(loc='upper left')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/engineered/temporal_features_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"  ✓ Temporal features plot saved: temporal_features_sample.png ({sample_ticker})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 18. Feature Engineering Summary Report\n",
    "# \n",
    "# **Generate a comprehensive summary report.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "{'='*70}\n",
    "FEATURE ENGINEERING SUMMARY REPORT\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*70}\n",
    "\n",
    "DATA OVERVIEW\n",
    "-------------\n",
    "Source File: RV_March2024.xlsx\n",
    "Date Range: {df_master['Date'].min().date()} to {df_master['Date'].max().date()}\n",
    "Trading Days: {df_master['Date'].nunique()}\n",
    "Companies: {df_master['Ticker'].nunique()}\n",
    "Total Observations: {len(df_master):,}\n",
    "\n",
    "FEATURE CATEGORIES\n",
    "------------------\n",
    "\"\"\"\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    summary_report += f\"\\n{category}: {len(features)} features\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "TOTAL FEATURES: {total_features}\n",
    "\n",
    "DATA SPLITS\n",
    "-----------\n",
    "Training Set: {len(train_data):,} obs ({len(train_data)/len(df_master)*100:.1f}%)\n",
    "  Period: {train_data['Date'].min().date()} to {train_data['Date'].max().date()}\n",
    "\n",
    "Validation Set: {len(val_data):,} obs ({len(val_data)/len(df_master)*100:.1f}%)\n",
    "  Period: {val_data['Date'].min().date()} to {val_data['Date'].max().date()}\n",
    "\n",
    "Test Set: {len(test_data):,} obs ({len(test_data)/len(df_master)*100:.1f}%)\n",
    "  Period: {test_data['Date'].min().date()} to {test_data['Date'].max().date()}\n",
    "\n",
    "DATA QUALITY\n",
    "------------\n",
    "Missing Data (original): 4.24% (handled via imputation)\n",
    "Missing Data (post-processing): {df_master.isna().sum().sum() / (len(df_master) * len(df_master.columns)) * 100:.2f}%\n",
    "Infinite Values: 0 (replaced with NaN)\n",
    "Variance Decomposition Error: {decomp_error.mean():.6f} (mean absolute)\n",
    "\n",
    "KEY ENGINEERED FEATURES\n",
    "-----------------------\n",
    "Temporal Features:\n",
    "  - Lags: 1, 5, 10, 20 days\n",
    "  - Rolling statistics: 5, 20, 60-day windows (mean, std, min, max)\n",
    "  - Momentum indicators: 5-day, 20-day\n",
    "\n",
    "Decomposition Features:\n",
    "  - Good/Bad variance ratios\n",
    "  - Jump indicators (threshold: 20%)\n",
    "  - Jump frequency and intensity\n",
    "  - Cross-frequency comparisons\n",
    "\n",
    "Cross-Sectional Features:\n",
    "  - Market-wide statistics (mean, median, std, min, max)\n",
    "  - Relative volatility vs market\n",
    "  - Volatility z-scores and rankings\n",
    "  - Market dispersion measures\n",
    "\n",
    "Frequency Features:\n",
    "  - 1-min vs 5-min ratios\n",
    "  - Microstructure noise estimates\n",
    "  - Frequency consistency measures\n",
    "\n",
    "Calendar Features:\n",
    "  - Year, month, quarter, day of week\n",
    "  - Monday/Friday indicators\n",
    "  - Month-end effects\n",
    "\n",
    "Data Quality Features:\n",
    "  - Missingness indicators\n",
    "  - Data completeness scores\n",
    "  - Consecutive missing days\n",
    "\n",
    "OUTPUT FILES\n",
    "------------\n",
    "Location: data/engineered/\n",
    "\n",
    "Datasets:\n",
    "  ✓ rv_features_full.parquet (complete dataset)\n",
    "  ✓ rv_features_full.csv (complete dataset)\n",
    "  ✓ rv_features_train.parquet (training set)\n",
    "  ✓ rv_features_val.parquet (validation set)\n",
    "  ✓ rv_features_test.parquet (test set)\n",
    "\n",
    "Metadata:\n",
    "  ✓ feature_metadata.csv (feature details)\n",
    "  ✓ feature_categories.csv (category mapping)\n",
    "\n",
    "Visualizations:\n",
    "  ✓ feature_correlations.png\n",
    "  ✓ feature_distributions.png\n",
    "  ✓ temporal_features_sample.png\n",
    "\n",
    "NEXT STEPS\n",
    "----------\n",
    "1. Load engineered data: pd.read_parquet('data/engineered/rv_features_train.parquet')\n",
    "2. Select features for modeling\n",
    "3. Apply feature scaling/normalization as needed\n",
    "4. Build and train predictive models\n",
    "5. Evaluate on validation set\n",
    "6. Final evaluation on held-out test set\n",
    "\n",
    "NOTES\n",
    "-----\n",
    "- Time-based split ensures no data leakage\n",
    "- All temporal features respect chronological order\n",
    "- Missing data handled systematically (forward fill + interpolation)\n",
    "- Features ready for machine learning without further preprocessing\n",
    "\n",
    "{'='*70}\n",
    "END OF REPORT\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report to file\n",
    "with open('data/engineered/feature_engineering_report.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(\"\\n  ✓ Summary report saved: feature_engineering_report.txt\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 19. Quick Data Loading Template (for Future Use)\n",
    "# \n",
    "# **Template code for loading engineered data in modeling notebooks.**\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADING TEMPLATE FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "template_code = \"\"\"\n",
    "# ============================================\n",
    "# TEMPLATE: Load Engineered Features\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_parquet('data/engineered/rv_features_train.parquet')\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_parquet('data/engineered/rv_features_val.parquet')\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_parquet('data/engineered/rv_features_test.parquet')\n",
    "\n",
    "# Load feature metadata\n",
    "feature_metadata = pd.read_csv('data/engineered/feature_metadata.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Total features: {len(train_df.columns)}\")\n",
    "\n",
    "# Define feature groups\n",
    "identifier_cols = ['Date', 'Ticker']\n",
    "target_col = 'RV'\n",
    "feature_cols = [c for c in train_df.columns if c not in identifier_cols + [target_col]]\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[target_col]\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(f\"\\\\nFeature matrix shape: {X_train.shape}\")\n",
    "print(f\"Target vector shape: {y_train.shape}\")\n",
    "\n",
    "# Optional: Select specific feature categories\n",
    "temporal_features = [c for c in feature_cols if 'lag' in c or 'roll' in c]\n",
    "decomp_features = [c for c in feature_cols if 'Good' in c or 'Bad' in c or 'jump' in c]\n",
    "market_features = [c for c in feature_cols if 'market' in c or 'rank' in c]\n",
    "\n",
    "# Ready for modeling!\n",
    "\"\"\"\n",
    "\n",
    "print(template_code)\n",
    "\n",
    "# Save template\n",
    "with open('data/engineered/loading_template.py', 'w') as f:\n",
    "    f.write(template_code)\n",
    "print(\"  ✓ Loading template saved: loading_template.py\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusion\n",
    "# \n",
    "# **Feature engineering complete!** \n",
    "# \n",
    "# We have successfully transformed raw high-frequency volatility data into a rich, modeling-ready dataset with:\n",
    "# - ✅ **{total_features} engineered features** across 8 categories\n",
    "# - ✅ **{len(df_master):,} observations** from 30 Dow Jones stocks\n",
    "# - ✅ **Time-based splits** (train/val/test) for proper evaluation\n",
    "# - ✅ **Clean data** with systematic missing data handling\n",
    "# - ✅ **Comprehensive documentation** and metadata\n",
    "# \n",
    "# **Key Highlights:**\n",
    "# 1. Temporal features capture volatility dynamics and persistence\n",
    "# 2. Decomposition features enable jump detection and risk analysis\n",
    "# 3. Cross-sectional features provide market context\n",
    "# 4. Calendar features capture seasonal patterns\n",
    "# 5. Data quality features track imputation and completeness\n",
    "# \n",
    "# **Ready for:** Time series forecasting, classification (jump detection), clustering, anomaly detection, and risk modeling.\n",
    "# \n",
    "# **Next Notebook:** `3_Model_Development.ipynb`\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Total features engineered: {total_features}\")\n",
    "print(f\"✓ Final dataset size: {len(df_master):,} observations\")\n",
    "print(f\"✓ Ready for modeling: train/val/test splits created\")\n",
    "print(f\"✓ All outputs saved to: data/engineered/\")\n",
    "print(f\"\\nNext step: Model Development (3_Model_Development.ipynb)\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_general]",
   "language": "python",
   "name": "conda-env-ds_general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
