{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c03a045",
   "metadata": {},
   "source": [
    "# Realized Variance (RV) Prediction - Models\n",
    "\n",
    "**Objective:** Predict next-day Realized Variance (RV) using engineered features from high-frequency volatility data\n",
    "\n",
    "**Dataset:**\n",
    "- 30 Dow Jones stocks (2003-2024)\n",
    "- 74 engineered features across 8 categories\n",
    "- Train: 2003-2018 | Validation: 2019-2021 | Test: 2022-2024\n",
    "\n",
    "**Approach:** Regression models (XGBoost, LightGBM, Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d655546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48b13a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Samples: 153,575\n",
      "Total Features: 74\n",
      "After creating next-day target: 153,486\n",
      "Dropped rows (last day per ticker): 89\n",
      "\n",
      "Sample of training data (with next-day RV target):\n",
      "        Date Ticker       RV   RV_t+1\n",
      "0 2003-01-02   AAPL   8.3082   6.5682\n",
      "1 2003-01-03   AAPL   6.5682   7.3444\n",
      "2 2003-01-06   AAPL   7.3444  10.0133\n",
      "3 2003-01-07   AAPL  10.0133   6.0982\n",
      "4 2003-01-08   AAPL   6.0982   5.5478\n",
      "5 2003-01-09   AAPL   5.5478   7.4340\n",
      "6 2003-01-10   AAPL   7.4340   6.9826\n",
      "7 2003-01-13   AAPL   6.9826   5.0252\n",
      "8 2003-01-14   AAPL   5.0252   5.5881\n",
      "9 2003-01-15   AAPL   5.5881   7.5931\n"
     ]
    }
   ],
   "source": [
    "# Load data from Feature Engineering output\n",
    "data_path = '../Feature Engineering Group 6/data/engineered/'\n",
    "\n",
    "train_df = pd.read_parquet(data_path + 'rv_features_train.parquet')\n",
    "val_df = pd.read_parquet(data_path + 'rv_features_val.parquet')\n",
    "test_df = pd.read_parquet(data_path + 'rv_features_test.parquet')\n",
    "\n",
    "print(f\"\\nOriginal Samples: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Total Features: {train_df.shape[1]}\")\n",
    "\n",
    "# Create next-day RV as target variable\n",
    "train_df['RV_t+1'] = train_df.groupby('Ticker')['RV'].shift(-1)\n",
    "val_df['RV_t+1'] = val_df.groupby('Ticker')['RV'].shift(-1)\n",
    "test_df['RV_t+1'] = test_df.groupby('Ticker')['RV'].shift(-1)\n",
    "\n",
    "# Drop rows where target is NaN (last day for each ticker in each split)\n",
    "train_df = train_df.dropna(subset=['RV_t+1'])\n",
    "val_df = val_df.dropna(subset=['RV_t+1'])\n",
    "test_df = test_df.dropna(subset=['RV_t+1'])\n",
    "\n",
    "print(f\"After creating next-day target: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Dropped rows (last day per ticker): {153575 - (len(train_df) + len(val_df) + len(test_df))}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of training data (with next-day RV target):\")\n",
    "print(train_df[['Date', 'Ticker', 'RV', 'RV_t+1']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cb1c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features for modeling: 71\n",
      "Target variable: RV_t+1 (next-day RV)\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "identifier_cols = ['Date', 'Ticker']\n",
    "target_col = 'RV_t+1'  # Next-day RV as target\n",
    "\n",
    "# Get all feature columns (exclude identifiers, original RV, and target)\n",
    "feature_cols = [c for c in train_df.columns if c not in identifier_cols + ['RV', target_col]]\n",
    "\n",
    "print(f\"Total features for modeling: {len(feature_cols)}\")\n",
    "print(f\"Target variable: {target_col} (next-day RV)\")\n",
    "\n",
    "# Categorize features for analysis\n",
    "temporal_features = [c for c in feature_cols if 'lag' in c or 'roll' in c or 'momentum' in c or 'volatility' in c]\n",
    "decomp_features = [c for c in feature_cols if 'Good' in c or 'Bad' in c or 'jump' in c or 'decomp' in c]\n",
    "market_features = [c for c in feature_cols if 'market' in c or 'rank' in c or 'vs_market' in c]\n",
    "freq_features = [c for c in feature_cols if 'freq' in c or 'microstructure' in c]\n",
    "calendar_features = ['year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year', \n",
    "                     'is_monday', 'is_friday', 'is_month_end']\n",
    "original_measures = ['BPV', 'Good', 'Bad', 'RQ', 'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297ecc5",
   "metadata": {},
   "source": [
    "## Feature Scaling & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaf72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (114029, 71)\n",
      "X_val: (22627, 71)\n",
      "X_test: (16830, 71)\n",
      "\n",
      "Log-transformed features: 33\n",
      "StandardScaled features: 57\n",
      "Unscaled features: 14\n"
     ]
    }
   ],
   "source": [
    "# Import scaling libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare X and y for each dataset (before scaling)\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_val = val_df[target_col].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_col].copy()\n",
    "\n",
    "# Handle missing values first (lag features at start of time series)\n",
    "missing_counts = X_train.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_counts) > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "log_transform_features = [\n",
    "    # Original volatility measures - always positive\n",
    "    'BPV', 'Good', 'Bad', 'RQ',\n",
    "    # 5-min measures - always positive\n",
    "    'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5',\n",
    "    # Lagged features - always positive\n",
    "    'RV_lag1', 'RV_lag5', 'RV_lag10', 'RV_lag20',\n",
    "    # Rolling statistics - always positive\n",
    "    'RV_roll_mean_5', 'RV_roll_std_5', 'RV_roll_min_5', 'RV_roll_max_5',\n",
    "    'RV_roll_mean_20', 'RV_roll_std_20', 'RV_roll_min_20', 'RV_roll_max_20',\n",
    "    'RV_roll_mean_60', 'RV_roll_std_60', 'RV_roll_min_60', 'RV_roll_max_60',\n",
    "    # Market-wide measures - always positive\n",
    "    'market_RV_mean', 'market_RV_median', 'market_RV_std', 'market_RV_min', 'market_RV_max',\n",
    "    # Other positive volatility features\n",
    "    'RV_volatility_20', 'jump_intensity', 'market_dispersion'\n",
    "]\n",
    "\n",
    "log_transform_features = [f for f in log_transform_features if f in feature_cols]\n",
    "\n",
    "# Apply log1p transformation\n",
    "for col in log_transform_features:\n",
    "    X_train[col] = np.log1p(X_train[col])\n",
    "    X_val[col] = np.log1p(X_val[col])\n",
    "    X_test[col] = np.log1p(X_test[col])\n",
    "\n",
    "nan_count = X_train.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "no_scale_features = [\n",
    "    'year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year',\n",
    "    'is_monday', 'is_friday', 'is_month_end',\n",
    "    'jump_indicator',\n",
    "    'RV_is_missing', 'BPV_is_missing', 'Good_is_missing', 'Bad_is_missing',\n",
    "]\n",
    "\n",
    "# Features to scale\n",
    "scale_features = [f for f in feature_cols if f not in no_scale_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[scale_features])\n",
    "\n",
    "# Transform all datasets\n",
    "X_train[scale_features] = scaler.transform(X_train[scale_features])\n",
    "X_val[scale_features] = scaler.transform(X_val[scale_features])\n",
    "X_test[scale_features] = scaler.transform(X_test[scale_features])\n",
    "\n",
    "y_train_original = y_train.copy()\n",
    "y_val_original = y_val.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)\n",
    "y_test = np.log1p(y_test)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"\\nLog-transformed features: {len(log_transform_features)}\")\n",
    "print(f\"StandardScaled features: {len(scale_features)}\")\n",
    "print(f\"Unscaled features: {len(no_scale_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15ae5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf_log = rf_model.predict(X_train)\n",
    "y_val_pred_rf_log = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(y_true_log, y_pred_log):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2 = r2_score(y_true_log, y_pred_log)\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "rf_train_metrics = evaluate_model(y_train, y_train_pred_rf_log)\n",
    "rf_val_metrics = evaluate_model(y_val, y_val_pred_rf_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d732817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_train_pred_xgb_log = xgb_model.predict(X_train)\n",
    "y_val_pred_xgb_log = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate in log-space\n",
    "xgb_train_metrics = evaluate_model(y_train, y_train_pred_xgb_log)\n",
    "xgb_val_metrics = evaluate_model(y_val, y_val_pred_xgb_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a97e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Add early stopping to prevent overfitting\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[\n",
    "        lgbm.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgbm.log_evaluation(period=0)\n",
    "    ])\n",
    "\n",
    "y_train_pred_lgbm_log = lgbm_model.predict(X_train)\n",
    "y_val_pred_lgbm_log = lgbm_model.predict(X_val)\n",
    "\n",
    "# Evaluate in log-space (where model was trained)\n",
    "lgbm_train_metrics = evaluate_model(y_train, y_train_pred_lgbm_log)\n",
    "lgbm_val_metrics = evaluate_model(y_val, y_val_pred_lgbm_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1cec099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison (Training & Validation):\n",
      "        Model  Train_RMSE  Train_R2  Train_MAE  Val_RMSE  Val_R2  Val_MAE\n",
      "Random Forest      0.1934    0.8957     0.1350    0.3144  0.7312   0.2137\n",
      "      XGBoost      0.2268    0.8566     0.1609    0.3139  0.7320   0.2141\n",
      "     LightGBM      0.2557    0.8177     0.1806    0.3037  0.7492   0.2119\n",
      "\n",
      "Best Model (by Validation R²): LightGBM\n",
      "  Training R²: 0.8177\n",
      "  Validation R²: 0.7492\n",
      "  Validation RMSE (log-space): 0.3037\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'XGBoost', 'LightGBM'],\n",
    "    'Train_RMSE': [rf_train_metrics['RMSE'], xgb_train_metrics['RMSE'], lgbm_train_metrics['RMSE']],\n",
    "    'Train_R2': [rf_train_metrics['R2'], xgb_train_metrics['R2'], lgbm_train_metrics['R2']],\n",
    "    'Train_MAE': [rf_train_metrics['MAE'], xgb_train_metrics['MAE'], lgbm_train_metrics['MAE']],\n",
    "    'Val_RMSE': [rf_val_metrics['RMSE'], xgb_val_metrics['RMSE'], lgbm_val_metrics['RMSE']],\n",
    "    'Val_R2': [rf_val_metrics['R2'], xgb_val_metrics['R2'], lgbm_val_metrics['R2']],\n",
    "    'Val_MAE': [rf_val_metrics['MAE'], xgb_val_metrics['MAE'], lgbm_val_metrics['MAE']]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison (Training & Validation):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model based on validation R2\n",
    "best_model_idx = comparison_df['Val_R2'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest Model (by Validation R²): {best_model_name}\")\n",
    "print(f\"  Training R²: {comparison_df.loc[best_model_idx, 'Train_R2']:.4f}\")\n",
    "print(f\"  Validation R²: {comparison_df.loc[best_model_idx, 'Val_R2']:.4f}\")\n",
    "print(f\"  Validation RMSE (log-space): {comparison_df.loc[best_model_idx, 'Val_RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea327e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6f39dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "\n",
      "Best parameters found:\n",
      "  subsample: 0.7\n",
      "  reg_lambda: 0.1\n",
      "  reg_alpha: 0.5\n",
      "  n_estimators: 300\n",
      "  min_child_samples: 20\n",
      "  max_depth: 8\n",
      "  learning_rate: 0.1\n",
      "  colsample_bytree: 0.9\n",
      "\n",
      "Best CV R² score: 0.8396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "lgbm_base = LGBMRegressor(n_estimators=1000, random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=15,\n",
    "    cv=tscv,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV R² score: {random_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beadf92",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b93f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance (log-space):\n",
      "  RMSE: 0.2095\n",
      "  MAE:  0.1458\n",
      "  R²:   0.8777\n",
      "\n",
      "Validation Set Performance (log-space):\n",
      "  RMSE: 0.3133\n",
      "  MAE:  0.2195\n",
      "  R²:   0.7331\n",
      "\n",
      "Test Set Performance (log-space):\n",
      "  RMSE: 0.2739\n",
      "  MAE:  0.1918\n",
      "  R²:   0.6269\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: BASELINE vs TUNED\n",
      "======================================================================\n",
      "\n",
      "Performance Comparison:\n",
      "            Model  Train_R2  Train_RMSE  Val_R2  Val_RMSE  Test_R2  Test_RMSE\n",
      "Baseline LightGBM    0.8177      0.2557  0.7492    0.3037      NaN        NaN\n",
      "   Tuned LightGBM    0.8777      0.2095  0.7331    0.3133   0.6269     0.2739\n"
     ]
    }
   ],
   "source": [
    "best_lgbm_model = random_search.best_estimator_\n",
    "\n",
    "y_train_pred_tuned_log = best_lgbm_model.predict(X_train)\n",
    "y_val_pred_tuned_log = best_lgbm_model.predict(X_val)\n",
    "y_test_pred_tuned_log = best_lgbm_model.predict(X_test)\n",
    "\n",
    "train_metrics_tuned = evaluate_model(y_train, y_train_pred_tuned_log)\n",
    "val_metrics_tuned = evaluate_model(y_val, y_val_pred_tuned_log)\n",
    "test_metrics_tuned = evaluate_model(y_test, y_test_pred_tuned_log)\n",
    "\n",
    "print(\"\\nTraining Set Performance (log-space):\")\n",
    "print(f\"  RMSE: {train_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {train_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {train_metrics_tuned['R2']:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set Performance (log-space):\")\n",
    "print(f\"  RMSE: {val_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {val_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {val_metrics_tuned['R2']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance (log-space):\")\n",
    "print(f\"  RMSE: {test_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {test_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {test_metrics_tuned['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: BASELINE vs TUNED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Model': ['Baseline LightGBM', 'Tuned LightGBM'],\n",
    "    'Train_R2': [lgbm_train_metrics['R2'], train_metrics_tuned['R2']],\n",
    "    'Train_RMSE': [lgbm_train_metrics['RMSE'], train_metrics_tuned['RMSE']],\n",
    "    'Val_R2': [lgbm_val_metrics['R2'], val_metrics_tuned['R2']],\n",
    "    'Val_RMSE': [lgbm_val_metrics['RMSE'], val_metrics_tuned['RMSE']],\n",
    "    'Test_R2': [None, test_metrics_tuned['R2']],\n",
    "    'Test_RMSE': [None, test_metrics_tuned['RMSE']]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_final.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
