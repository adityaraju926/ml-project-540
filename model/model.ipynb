{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c03a045",
   "metadata": {},
   "source": [
    "# Realized Variance (RV) Prediction - Models\n",
    "\n",
    "**Objective:** Predict next-day Realized Variance (RV) using engineered features from high-frequency volatility data\n",
    "\n",
    "**Dataset:**\n",
    "- 30 Dow Jones stocks (2003-2024)\n",
    "- 74 engineered features across 8 categories\n",
    "- Train: 2003-2018 | Validation: 2019-2021 | Test: 2022-2024\n",
    "\n",
    "**Approach:** Regression models (XGBoost, LightGBM, Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d655546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b13a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Samples: 153,575\n",
      "Total Features: 74\n",
      "\n",
      "Sample of training data:\n",
      "        Date Ticker       RV     BPV    Good     Bad        RQ    RV_5  \\\n",
      "0 2003-01-02   AAPL   8.3082  6.2018  5.4820  2.8262  263.0252  6.4939   \n",
      "1 2003-01-03   AAPL   6.5682  5.3314  3.3633  3.2048   98.4764  6.5745   \n",
      "2 2003-01-06   AAPL   7.3444  6.1792  3.8547  3.4897  301.8829  5.9923   \n",
      "3 2003-01-07   AAPL  10.0133  9.1303  5.5418  4.4715  215.2682  9.5007   \n",
      "4 2003-01-08   AAPL   6.0982  4.9211  3.2482  2.8500   97.9670  4.9405   \n",
      "\n",
      "    BPV_5  Good_5   Bad_5      RQ_5  RV_lag1  RV_lag5  RV_lag10  RV_lag20  \\\n",
      "0  3.7720  5.1023  1.3916  152.7294      NaN      NaN       NaN       NaN   \n",
      "1  5.8933  3.6380  2.9365   62.0543   8.3082      NaN       NaN       NaN   \n",
      "2  4.4432  3.8791  2.1132  180.2133   6.5682      NaN       NaN       NaN   \n",
      "3  7.1200  5.7827  3.7179  197.3819   7.3444      NaN       NaN       NaN   \n",
      "4  5.4333  2.4732  2.4674   35.7264  10.0133      NaN       NaN       NaN   \n",
      "\n",
      "   RV_roll_mean_5  RV_roll_std_5  RV_roll_min_5  RV_roll_max_5  \\\n",
      "0          8.3082            NaN         8.3082         8.3082   \n",
      "1          7.4382         1.2304         6.5682         8.3082   \n",
      "2          7.4069         0.8717         6.5682         8.3082   \n",
      "3          8.0585         1.4849         6.5682        10.0133   \n",
      "4          7.6665         1.5564         6.0982        10.0133   \n",
      "\n",
      "   RV_roll_mean_20  RV_roll_std_20  RV_roll_min_20  RV_roll_max_20  \\\n",
      "0           8.3082             NaN          8.3082          8.3082   \n",
      "1           7.4382          1.2304          6.5682          8.3082   \n",
      "2           7.4069          0.8717          6.5682          8.3082   \n",
      "3           8.0585          1.4849          6.5682         10.0133   \n",
      "4           7.6665          1.5564          6.0982         10.0133   \n",
      "\n",
      "   RV_roll_mean_60  RV_roll_std_60  RV_roll_min_60  RV_roll_max_60  \\\n",
      "0           8.3082             NaN          8.3082          8.3082   \n",
      "1           7.4382          1.2304          6.5682          8.3082   \n",
      "2           7.4069          0.8717          6.5682          8.3082   \n",
      "3           8.0585          1.4849          6.5682         10.0133   \n",
      "4           7.6665          1.5564          6.0982         10.0133   \n",
      "\n",
      "   RV_momentum_5  RV_momentum_20  RV_volatility_20  Good_Bad_ratio  Bad_pct  \\\n",
      "0            NaN             NaN               NaN          1.9397   0.3402   \n",
      "1            NaN             NaN            1.2304          1.0495   0.4879   \n",
      "2            NaN             NaN            0.8717          1.1046   0.4752   \n",
      "3            NaN             NaN            1.4849          1.2394   0.4466   \n",
      "4            NaN             NaN            1.5564          1.1397   0.4673   \n",
      "\n",
      "   Good_pct  decomp_check  jump_indicator  jump_freq_20  jump_intensity  \\\n",
      "0    0.6598           1.0               1           1.0          2.8262   \n",
      "1    0.5121           1.0               1           1.0          3.2048   \n",
      "2    0.5248           1.0               1           1.0          3.4897   \n",
      "3    0.5534           1.0               1           1.0          4.4715   \n",
      "4    0.5327           1.0               1           1.0          2.8500   \n",
      "\n",
      "   Bad_pct_5  jump_diff_freq  market_RV_mean  market_RV_median  market_RV_std  \\\n",
      "0     0.2143          0.1259          4.3852            3.3892         2.6208   \n",
      "1     0.4466          0.0413          4.3615            3.6695         2.9704   \n",
      "2     0.3527          0.1225          2.9021            2.4354         1.7204   \n",
      "3     0.3913          0.0552          4.3074            3.8024         2.6821   \n",
      "4     0.4994         -0.0321          3.4625            3.2724         1.6657   \n",
      "\n",
      "   market_RV_min  market_RV_max  market_Bad_pct_mean  market_Bad_pct_median  \\\n",
      "0         1.1967        10.5470               0.4047                 0.4024   \n",
      "1         1.1625        14.1988               0.4966                 0.4869   \n",
      "2         0.9766         7.3444               0.4365                 0.4304   \n",
      "3         1.3674        11.7349               0.5231                 0.5239   \n",
      "4         0.9412         7.3627               0.5413                 0.5424   \n",
      "\n",
      "   market_jump_count  RV_vs_market  RV_zscore  RV_rank  market_dispersion  \\\n",
      "0                 26        1.8946     1.4968   0.9231             9.3503   \n",
      "1                 26        1.5059     0.7429   0.8077            13.0362   \n",
      "2                 26        2.5307     2.5822   1.0000             6.3678   \n",
      "3                 26        2.3247     2.1274   0.9615            10.3675   \n",
      "4                 26        1.7612     1.5823   0.9231             6.4215   \n",
      "\n",
      "   market_CV  RV_freq_ratio  BPV_freq_ratio  Good_freq_ratio  Bad_freq_ratio  \\\n",
      "0     0.5977         1.2794          1.6442           1.0744          2.0309   \n",
      "1     0.6811         0.9990          0.9046           0.9245          1.0914   \n",
      "2     0.5928         1.2256          1.3907           0.9937          1.6514   \n",
      "3     0.6227         1.0540          1.2824           0.9583          1.2027   \n",
      "4     0.4811         1.2343          0.9057           1.3134          1.1551   \n",
      "\n",
      "   microstructure_noise  freq_consistency  year  month  quarter  day_of_week  \\\n",
      "0                1.8143            0.2794  2003      1        1            3   \n",
      "1               -0.0063            0.0010  2003      1        1            4   \n",
      "2                1.3522            0.2256  2003      1        1            0   \n",
      "3                0.5127            0.0540  2003      1        1            1   \n",
      "4                1.1577            0.2343  2003      1        1            2   \n",
      "\n",
      "   day_of_month  week_of_year  is_monday  is_friday  is_month_end  \\\n",
      "0             2             1          0          0             0   \n",
      "1             3             1          0          1             0   \n",
      "2             6             2          1          0             0   \n",
      "3             7             2          0          0             0   \n",
      "4             8             2          0          0             0   \n",
      "\n",
      "   RV_is_missing  BPV_is_missing  Good_is_missing  Bad_is_missing  \\\n",
      "0              0               0                0               0   \n",
      "1              0               0                0               0   \n",
      "2              0               0                0               0   \n",
      "3              0               0                0               0   \n",
      "4              0               0                0               0   \n",
      "\n",
      "   data_completeness_20  consec_missing  \n",
      "0                   1.0               0  \n",
      "1                   1.0               0  \n",
      "2                   1.0               0  \n",
      "3                   1.0               0  \n",
      "4                   1.0               0  \n"
     ]
    }
   ],
   "source": [
    "# Load data from Feature Engineering output\n",
    "data_path = '../Feature Engineering Group 6/data/engineered/'\n",
    "\n",
    "train_df = pd.read_parquet(data_path + 'rv_features_train.parquet')\n",
    "val_df = pd.read_parquet(data_path + 'rv_features_val.parquet')\n",
    "test_df = pd.read_parquet(data_path + 'rv_features_test.parquet')\n",
    "\n",
    "print(f\"\\nTotal Samples: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Total Features: {train_df.shape[1]}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cb1c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features for modeling: 71\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "identifier_cols = ['Date', 'Ticker']\n",
    "target_col = 'RV'\n",
    "\n",
    "# Get all feature columns (exclude identifiers and target)\n",
    "feature_cols = [c for c in train_df.columns if c not in identifier_cols + [target_col]]\n",
    "\n",
    "print(f\"Total features for modeling: {len(feature_cols)}\")\n",
    "\n",
    "# Categorize features for analysis\n",
    "temporal_features = [c for c in feature_cols if 'lag' in c or 'roll' in c or 'momentum' in c or 'volatility' in c]\n",
    "decomp_features = [c for c in feature_cols if 'Good' in c or 'Bad' in c or 'jump' in c or 'decomp' in c]\n",
    "market_features = [c for c in feature_cols if 'market' in c or 'rank' in c or 'vs_market' in c]\n",
    "freq_features = [c for c in feature_cols if 'freq' in c or 'microstructure' in c]\n",
    "calendar_features = ['year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year', \n",
    "                     'is_monday', 'is_friday', 'is_month_end']\n",
    "original_measures = ['BPV', 'Good', 'Bad', 'RQ', 'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297ecc5",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling & Transformation\n",
    "\n",
    "**Rationale:**\n",
    "- Volatility measures are heavily right-skewed and dominated by extreme events (COVID crash, rate hikes)\n",
    "\n",
    "**Approach:**\n",
    "1. **Log transformation** for right-skewed volatility measures (RV, BPV, RQ, Good, Bad, etc.)\n",
    "2. **StandardScaler** for all continuous features to normalize scale\n",
    "3. **No scaling** for binary/categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48aaf72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (114058, 71)\n",
      "X_val: (22657, 71)\n",
      "X_test: (16860, 71)\n",
      "\n",
      "Log-transformed features: 33\n",
      "StandardScaled features: 57\n",
      "Unscaled features: 14\n",
      "Target variable (RV): log1p transformed\n"
     ]
    }
   ],
   "source": [
    "# Import scaling libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare X and y for each dataset (before scaling)\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_val = val_df[target_col].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_col].copy()\n",
    "\n",
    "# Handle missing values first (lag features at start of time series)\n",
    "missing_counts = X_train.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_counts) > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "log_transform_features = [\n",
    "    # Original volatility measures (1-min) - always positive\n",
    "    'BPV', 'Good', 'Bad', 'RQ',\n",
    "    # 5-min measures - always positive\n",
    "    'RV_5', 'BPV_5', 'Good_5', 'Bad_5', 'RQ_5',\n",
    "    # Lagged features - always positive (RV is always positive)\n",
    "    'RV_lag1', 'RV_lag5', 'RV_lag10', 'RV_lag20',\n",
    "    # Rolling statistics - always positive\n",
    "    'RV_roll_mean_5', 'RV_roll_std_5', 'RV_roll_min_5', 'RV_roll_max_5',\n",
    "    'RV_roll_mean_20', 'RV_roll_std_20', 'RV_roll_min_20', 'RV_roll_max_20',\n",
    "    'RV_roll_mean_60', 'RV_roll_std_60', 'RV_roll_min_60', 'RV_roll_max_60',\n",
    "    # Market-wide measures - always positive\n",
    "    'market_RV_mean', 'market_RV_median', 'market_RV_std', 'market_RV_min', 'market_RV_max',\n",
    "    # Other positive volatility features\n",
    "    'RV_volatility_20', 'jump_intensity', 'market_dispersion'\n",
    "    # EXCLUDED: 'RV_momentum_5', 'RV_momentum_20' (can be negative)\n",
    "    # EXCLUDED: 'RV_zscore' (can be negative)\n",
    "    # EXCLUDED: 'microstructure_noise' (can be negative)\n",
    "    # EXCLUDED: 'jump_diff_freq' (can be negative)\n",
    "]\n",
    "\n",
    "# Filter to only include features that exist in the dataset\n",
    "log_transform_features = [f for f in log_transform_features if f in feature_cols]\n",
    "\n",
    "# Apply log1p transformation\n",
    "for col in log_transform_features:\n",
    "    X_train[col] = np.log1p(X_train[col])\n",
    "    X_val[col] = np.log1p(X_val[col])\n",
    "    X_test[col] = np.log1p(X_test[col])\n",
    "\n",
    "# Verify no NaN values\n",
    "nan_count = X_train.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "no_scale_features = [\n",
    "    'year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year',\n",
    "    'is_monday', 'is_friday', 'is_month_end',\n",
    "    'jump_indicator',  # Binary\n",
    "    'RV_is_missing', 'BPV_is_missing', 'Good_is_missing', 'Bad_is_missing',  # Binary\n",
    "]\n",
    "\n",
    "# Features to scale\n",
    "scale_features = [f for f in feature_cols if f not in no_scale_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[scale_features])\n",
    "\n",
    "# Transform all datasets\n",
    "X_train[scale_features] = scaler.transform(X_train[scale_features])\n",
    "X_val[scale_features] = scaler.transform(X_val[scale_features])\n",
    "X_test[scale_features] = scaler.transform(X_test[scale_features])\n",
    "\n",
    "y_train_original = y_train.copy()\n",
    "y_val_original = y_val.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)\n",
    "y_test = np.log1p(y_test)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"\\nLog-transformed features: {len(log_transform_features)}\")\n",
    "print(f\"StandardScaled features: {len(scale_features)}\")\n",
    "print(f\"Unscaled features: {len(no_scale_features)}\")\n",
    "print(f\"Target variable (RV): log1p transformed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ae5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions (in log-space)\n",
    "y_train_pred_rf_log = rf_model.predict(X_train)\n",
    "y_val_pred_rf_log = rf_model.predict(X_val)\n",
    "\n",
    "# Inverse transform predictions back to original scale\n",
    "y_train_pred_rf = np.expm1(y_train_pred_rf_log)\n",
    "y_val_pred_rf = np.expm1(y_val_pred_rf_log)\n",
    "\n",
    "# Evaluate on all sets (using original scale)\n",
    "def evaluate_model(y_true_original, y_pred_original):\n",
    "    \"\"\"Evaluate model performance on original (non-transformed) scale\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n",
    "    mae = mean_absolute_error(y_true_original, y_pred_original)\n",
    "    r2 = r2_score(y_true_original, y_pred_original)\n",
    "    mape = mean_absolute_percentage_error(y_true_original, y_pred_original) * 100\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "rf_train_metrics = evaluate_model(y_train_original, y_train_pred_rf)\n",
    "rf_val_metrics = evaluate_model(y_val_original, y_val_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d732817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions (in log-space)\n",
    "y_train_pred_xgb_log = xgb_model.predict(X_train)\n",
    "y_val_pred_xgb_log = xgb_model.predict(X_val)\n",
    "\n",
    "# Inverse transform predictions back to original scale\n",
    "y_train_pred_xgb = np.expm1(y_train_pred_xgb_log)\n",
    "y_val_pred_xgb = np.expm1(y_val_pred_xgb_log)\n",
    "\n",
    "# Evaluate\n",
    "xgb_train_metrics = evaluate_model(y_train_original, y_train_pred_xgb)\n",
    "xgb_val_metrics = evaluate_model(y_val_original, y_val_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a97e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[])\n",
    "\n",
    "# Make predictions (in log-space)\n",
    "y_train_pred_lgbm_log = lgbm_model.predict(X_train)\n",
    "y_val_pred_lgbm_log = lgbm_model.predict(X_val)\n",
    "\n",
    "# Inverse transform predictions back to original scale\n",
    "y_train_pred_lgbm = np.expm1(y_train_pred_lgbm_log)\n",
    "y_val_pred_lgbm = np.expm1(y_val_pred_lgbm_log)\n",
    "\n",
    "# Evaluate\n",
    "lgbm_train_metrics = evaluate_model(y_train_original, y_train_pred_lgbm)\n",
    "lgbm_val_metrics = evaluate_model(y_val_original, y_val_pred_lgbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1cec099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison (Training & Validation):\n",
      "        Model  Train_RMSE  Train_R2  Train_MAE  Val_RMSE  Val_R2  Val_MAE\n",
      "Random Forest      1.1079    0.9706     0.0285    0.3998  0.9967   0.0379\n",
      "      XGBoost      1.1414    0.9688     0.0700    1.1286  0.9734   0.1086\n",
      "     LightGBM      1.2583    0.9621     0.0644    1.1307  0.9733   0.1064\n",
      "\n",
      "Best Model (by Validation R²): Random Forest\n",
      "  Training R²: 0.9706\n",
      "  Validation R²: 0.9967\n",
      "  Validation RMSE: 0.3998\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'XGBoost', 'LightGBM'],\n",
    "    'Train_RMSE': [rf_train_metrics['RMSE'], xgb_train_metrics['RMSE'], lgbm_train_metrics['RMSE']],\n",
    "    'Train_R2': [rf_train_metrics['R2'], xgb_train_metrics['R2'], lgbm_train_metrics['R2']],\n",
    "    'Train_MAE': [rf_train_metrics['MAE'], xgb_train_metrics['MAE'], lgbm_train_metrics['MAE']],\n",
    "    'Val_RMSE': [rf_val_metrics['RMSE'], xgb_val_metrics['RMSE'], lgbm_val_metrics['RMSE']],\n",
    "    'Val_R2': [rf_val_metrics['R2'], xgb_val_metrics['R2'], lgbm_val_metrics['R2']],\n",
    "    'Val_MAE': [rf_val_metrics['MAE'], xgb_val_metrics['MAE'], lgbm_val_metrics['MAE']]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison (Training & Validation):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model based on validation R2\n",
    "best_model_idx = comparison_df['Val_R2'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest Model (by Validation R²): {best_model_name}\")\n",
    "print(f\"  Training R²: {comparison_df.loc[best_model_idx, 'Train_R2']:.4f}\")\n",
    "print(f\"  Validation R²: {comparison_df.loc[best_model_idx, 'Val_R2']:.4f}\")\n",
    "print(f\"  Validation RMSE: {comparison_df.loc[best_model_idx, 'Val_RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea327e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6f39dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "\n",
      "Best parameters found:\n",
      "  n_estimators: 100\n",
      "  min_samples_split: 5\n",
      "  min_samples_leaf: 2\n",
      "  max_features: sqrt\n",
      "  max_depth: 15\n",
      "\n",
      "Best CV R² score: 0.9989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize base model\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=0)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=2,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV R² score: {random_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beadf92",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b93f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "  RMSE: 1.2510\n",
      "  MAE:  0.0361\n",
      "  R²:   0.9625\n",
      "  MAPE: 1.00%\n",
      "\n",
      "Validation Set Performance:\n",
      "  RMSE: 0.4959\n",
      "  MAE:  0.0561\n",
      "  R²:   0.9949\n",
      "  MAPE: 1.57%\n",
      "\n",
      "Test Set Performance:\n",
      "  RMSE: 1.0132\n",
      "  MAE:  0.0468\n",
      "  R²:   0.8632\n",
      "  MAPE: 1.42%\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: BASELINE vs TUNED\n",
      "======================================================================\n",
      "\n",
      "Performance Comparison:\n",
      "      Model  Train_R2  Train_RMSE  Val_R2  Val_RMSE  Test_R2  Test_RMSE\n",
      "Baseline RF    0.9706      1.1079  0.9967    0.3998      NaN        NaN\n",
      "   Tuned RF    0.9625      1.2510  0.9949    0.4959   0.8632     1.0132\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "y_train_pred_tuned_log = best_rf_model.predict(X_train)\n",
    "y_val_pred_tuned_log = best_rf_model.predict(X_val)\n",
    "y_test_pred_tuned_log = best_rf_model.predict(X_test)\n",
    "\n",
    "y_train_pred_tuned = np.expm1(y_train_pred_tuned_log)\n",
    "y_val_pred_tuned = np.expm1(y_val_pred_tuned_log)\n",
    "y_test_pred_tuned = np.expm1(y_test_pred_tuned_log)\n",
    "\n",
    "# Evaluate on all sets\n",
    "train_metrics_tuned = evaluate_model(y_train_original, y_train_pred_tuned)\n",
    "val_metrics_tuned = evaluate_model(y_val_original, y_val_pred_tuned)\n",
    "test_metrics_tuned = evaluate_model(y_test_original, y_test_pred_tuned)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  RMSE: {train_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {train_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {train_metrics_tuned['R2']:.4f}\")\n",
    "print(f\"  MAPE: {train_metrics_tuned['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"  RMSE: {val_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {val_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {val_metrics_tuned['R2']:.4f}\")\n",
    "print(f\"  MAPE: {val_metrics_tuned['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  RMSE: {test_metrics_tuned['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {test_metrics_tuned['MAE']:.4f}\")\n",
    "print(f\"  R²:   {test_metrics_tuned['R2']:.4f}\")\n",
    "print(f\"  MAPE: {test_metrics_tuned['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: BASELINE vs TUNED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Model': ['Baseline RF', 'Tuned RF'],\n",
    "    'Train_R2': [rf_train_metrics['R2'], train_metrics_tuned['R2']],\n",
    "    'Train_RMSE': [rf_train_metrics['RMSE'], train_metrics_tuned['RMSE']],\n",
    "    'Val_R2': [rf_val_metrics['R2'], val_metrics_tuned['R2']],\n",
    "    'Val_RMSE': [rf_val_metrics['RMSE'], val_metrics_tuned['RMSE']],\n",
    "    'Test_R2': [None, test_metrics_tuned['R2']],\n",
    "    'Test_RMSE': [None, test_metrics_tuned['RMSE']]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_final.to_string(index=False))\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
